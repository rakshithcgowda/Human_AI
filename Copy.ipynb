{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPb2JEWEa+q198MV5Rl+w5j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rakshithcgowda/Human_AI/blob/master/Copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y poppler-utils\n"
      ],
      "metadata": {
        "id": "Hjmgu7o9UN-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdf2image"
      ],
      "metadata": {
        "id": "dOFY3q7Wk9DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docx"
      ],
      "metadata": {
        "id": "mrQtqEualgnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rm -rf /content/jpg_images"
      ],
      "metadata": {
        "id": "bhEo6Ou6Ut9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Convert form PDF to JPG"
      ],
      "metadata": {
        "id": "4zXeFLx03b6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from pdf2image import convert_from_path, pdfinfo_from_path\n",
        "\n",
        "def convert_pdf_to_jpg(pdf_dir, jpg_dir, dpi=200):\n",
        "    \"\"\"\n",
        "    Convert PDF files to JPG images. Each page of a PDF will be converted to a separate JPG image.\n",
        "\n",
        "    This version processes one page at a time, which can help reduce memory usage.\n",
        "\n",
        "    Args:\n",
        "        pdf_dir (str): Directory containing PDF files.\n",
        "        jpg_dir (str): Directory where the JPG images will be saved.\n",
        "        dpi (int): Resolution for converting PDF pages.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(jpg_dir):\n",
        "        os.makedirs(jpg_dir)\n",
        "\n",
        "    for filename in os.listdir(pdf_dir):\n",
        "        if filename.lower().endswith(\".pdf\"):\n",
        "            pdf_path = os.path.join(pdf_dir, filename)\n",
        "            try:\n",
        "                # Retrieve PDF information to get the total number of pages.\n",
        "                info = pdfinfo_from_path(pdf_path, userpw=None)\n",
        "                total_pages = info.get(\"Pages\", 0)\n",
        "                print(f\"Processing '{filename}' with {total_pages} pages.\")\n",
        "\n",
        "                # Process each page individually to reduce memory usage.\n",
        "                for page_number in range(1, total_pages + 1):\n",
        "                    # Convert only one page at a time.\n",
        "                    pages = convert_from_path(\n",
        "                        pdf_path, dpi=dpi, first_page=page_number, last_page=page_number\n",
        "                    )\n",
        "                    for page in pages:\n",
        "                        jpg_filename = os.path.splitext(filename)[0] + f\"_page{page_number}.jpg\"\n",
        "                        jpg_path = os.path.join(jpg_dir, jpg_filename)\n",
        "                        page.save(jpg_path, \"JPEG\")\n",
        "                        print(f\"Saved JPG image to {jpg_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to process {pdf_path}: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Directories for PDF and JPG images.\n",
        "    pdf_folder = \"/content/pdf\"          # Directory with your PDF files\n",
        "    jpg_folder = \"/content/jpg_images\"     # Directory to save JPG images after conversion from PDF\n",
        "\n",
        "    # Convert PDF files to JPG images.\n",
        "    convert_pdf_to_jpg(pdf_folder, jpg_folder, dpi=200)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "iX3-qq0VRjtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convert JPG to Mask images"
      ],
      "metadata": {
        "id": "lKmmR67m3nB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def convert_jpgs_to_mask(jpg_dir, mask_dir, threshold=128):\n",
        "    \"\"\"\n",
        "    Convert JPG images to binary mask images using a simple threshold.\n",
        "\n",
        "    Args:\n",
        "        jpg_dir (str): Directory containing JPG images.\n",
        "        mask_dir (str): Directory where mask images will be saved.\n",
        "        threshold (int): Grayscale threshold value (0-255) to determine mask cutoff.\n",
        "                         Pixels with values above this threshold will be white (255),\n",
        "                         otherwise black (0).\n",
        "    \"\"\"\n",
        "    if not os.path.exists(mask_dir):\n",
        "        os.makedirs(mask_dir)\n",
        "\n",
        "    for filename in os.listdir(jpg_dir):\n",
        "        if filename.lower().endswith(\".jpg\"):\n",
        "            jpg_path = os.path.join(jpg_dir, filename)\n",
        "            try:\n",
        "                # Open the image and convert it to grayscale.\n",
        "                img = Image.open(jpg_path)\n",
        "                img_gray = img.convert(\"L\")\n",
        "\n",
        "                # Apply threshold to create a binary mask.\n",
        "                mask = img_gray.point(lambda p: 255 if p > threshold else 0)\n",
        "\n",
        "                # Save the mask image with a new name.\n",
        "                mask_filename = os.path.splitext(filename)[0] + \"_mask.png\"\n",
        "                mask_path = os.path.join(mask_dir, mask_filename)\n",
        "                mask.save(mask_path)\n",
        "                print(f\"Saved mask image to {mask_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to process {jpg_path}: {e}\")\n",
        "\n",
        "# -------------------------\n",
        "# Example usage:\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    jpg_folder = \"/content/jpg_images\"      # Directory with your existing JPG images\n",
        "    mask_output_folder = \"/content/mask_imgs\" # Directory to save the generated mask images\n",
        "    convert_jpgs_to_mask(jpg_folder, mask_output_folder, threshold=128)\n"
      ],
      "metadata": {
        "id": "CezfDQOVjicH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Specific Task I"
      ],
      "metadata": {
        "id": "MEQgBfsE3yBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "###########################################\n",
        "# Note on Image Quality and OCR\n",
        "###########################################\n",
        "# If you use this model (or a similar one) for text segmentation/OCR tasks,\n",
        "# be aware that low resolution, blurriness, poor contrast, and other image artifacts\n",
        "# can lead to poor predictions. To help improve performance, consider:\n",
        "#   - Using high-resolution images\n",
        "#   - Ensuring proper lighting and contrast\n",
        "#   - Cropping out extraneous background or noise\n",
        "#\n",
        "# The mismatch between input clarity and predicted output is often due to these factors.\n",
        "\n",
        "###########################################\n",
        "# Dataset Class\n",
        "###########################################\n",
        "class LayoutDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, image_transform=None, mask_transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.image_transform = image_transform\n",
        "        self.mask_transform = mask_transform\n",
        "        self.images = sorted([f for f in os.listdir(image_dir) if f.lower().endswith('.jpg')])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_filename = self.images[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_filename)\n",
        "        mask_filename = f\"{os.path.splitext(img_filename)[0]}_mask.png\"\n",
        "        mask_path = os.path.join(self.mask_dir, mask_filename)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if os.path.exists(mask_path):\n",
        "            mask = Image.open(mask_path).convert(\"L\")\n",
        "        else:\n",
        "            print(f\"Warning: Missing mask for {img_filename}. Using blank mask.\")\n",
        "            mask = Image.new(\"L\", image.size, 0)\n",
        "\n",
        "        image = self.image_transform(image) if self.image_transform else transforms.ToTensor()(image)\n",
        "        mask = self.mask_transform(mask) if self.mask_transform else transforms.ToTensor()(mask)\n",
        "        # Convert mask to binary (0 and 1)\n",
        "        mask = (mask > 0.5).long().squeeze(0)\n",
        "        return image, mask\n",
        "\n",
        "###########################################\n",
        "# Building Blocks for Advanced Model\n",
        "###########################################\n",
        "# Residual Double Convolution Block\n",
        "class ResDoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ResDoubleConv, self).__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        # Adjust dimensions if needed\n",
        "        self.res_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x if self.res_conv is None else self.res_conv(x)\n",
        "        out = self.double_conv(x)\n",
        "        out += residual\n",
        "        return self.relu(out)\n",
        "\n",
        "# Attention Block for Skip Connections\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, F_g, F_l, F_int):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        self.W_g = nn.Sequential(\n",
        "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(F_int)\n",
        "        )\n",
        "        self.W_x = nn.Sequential(\n",
        "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(F_int)\n",
        "        )\n",
        "        self.psi = nn.Sequential(\n",
        "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, g, x):\n",
        "        g1 = self.W_g(g)\n",
        "        x1 = self.W_x(x)\n",
        "        psi = self.relu(g1 + x1)\n",
        "        psi = self.psi(psi)\n",
        "        return x * psi\n",
        "\n",
        "# Up Block with Attention on Skip Connections\n",
        "class UpAttention(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        \"\"\"\n",
        "        in_channels: number of channels from concatenated decoder and encoder features.\n",
        "        out_channels: desired number of output channels.\n",
        "        \"\"\"\n",
        "        super(UpAttention, self).__init__()\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
        "        # For the attention gate, assume both the upsampled feature and skip connection have half of in_channels each.\n",
        "        self.attention = AttentionBlock(F_g=in_channels // 2, F_l=in_channels // 2, F_int=in_channels // 4)\n",
        "        # After concatenation of features, use a residual double conv block.\n",
        "        self.conv = ResDoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x_up, x_skip):\n",
        "        x_up = self.up(x_up)\n",
        "        # Ensure sizes match by padding if necessary.\n",
        "        diffY = x_skip.size()[2] - x_up.size()[2]\n",
        "        diffX = x_skip.size()[3] - x_up.size()[3]\n",
        "        x_up = F.pad(x_up, [diffX // 2, diffX - diffX // 2,\n",
        "                            diffY // 2, diffY - diffY // 2])\n",
        "        # Refine the skip connection with attention.\n",
        "        x_skip = self.attention(g=x_up, x=x_skip)\n",
        "        # Concatenate along the channel dimension.\n",
        "        x = torch.cat([x_skip, x_up], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "# Positional Encoding for Transformer\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=1000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)  # [max_len, d_model]\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # [max_len, 1]\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(1)  # [max_len, 1, d_model]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [seq_len, batch_size, d_model]\n",
        "        seq_len = x.size(0)\n",
        "        return x + self.pe[:seq_len]\n",
        "\n",
        "###########################################\n",
        "# AdvancedTransUNet: Refined Architecture\n",
        "###########################################\n",
        "class AdvancedTransUNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=True, transformer_layers=2, nhead=8):\n",
        "        super(AdvancedTransUNet, self).__init__()\n",
        "        # Encoder with residual blocks\n",
        "        self.inc = ResDoubleConv(n_channels, 64)         # [B, 64, H, W]\n",
        "        self.down1 = nn.Sequential(nn.MaxPool2d(2), ResDoubleConv(64, 128))   # [B, 128, H/2, W/2]\n",
        "        self.down2 = nn.Sequential(nn.MaxPool2d(2), ResDoubleConv(128, 256))  # [B, 256, H/4, W/4]\n",
        "        self.down3 = nn.Sequential(nn.MaxPool2d(2), ResDoubleConv(256, 512))  # [B, 512, H/8, W/8]\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = nn.Sequential(nn.MaxPool2d(2), ResDoubleConv(512, 1024 // factor))  # [B, 1024//factor, H/16, W/16]\n",
        "\n",
        "        self.feature_dim = 1024 // factor  # Bottleneck channels\n",
        "\n",
        "        # Projection layer (if needed) and positional encoding before transformer\n",
        "        self.transformer_input_proj = nn.Conv2d(self.feature_dim, self.feature_dim, kernel_size=1)\n",
        "        self.pos_encoder = PositionalEncoding(d_model=self.feature_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=self.feature_dim, nhead=nhead)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=transformer_layers)\n",
        "\n",
        "        # Decoder with Attention-based Up Blocks\n",
        "        # Note: in_channels here is the sum of channels from upsampled feature and corresponding encoder skip.\n",
        "        self.up1 = UpAttention(in_channels=1024, out_channels=256, bilinear=bilinear)  # x5 (512) + x4 (512) = 1024 -> 256\n",
        "        self.up2 = UpAttention(in_channels=512, out_channels=128, bilinear=bilinear)   # 256 + 256 = 512 -> 128\n",
        "        self.up3 = UpAttention(in_channels=256, out_channels=64, bilinear=bilinear)    # 128 + 128 = 256 -> 64\n",
        "        self.up4 = UpAttention(in_channels=128, out_channels=64, bilinear=bilinear)    # 64 + 64 = 128 -> 64\n",
        "\n",
        "        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder path\n",
        "        x1 = self.inc(x)       # [B, 64, H, W]\n",
        "        x2 = self.down1(x1)    # [B, 128, H/2, W/2]\n",
        "        x3 = self.down2(x2)    # [B, 256, H/4, W/4]\n",
        "        x4 = self.down3(x3)    # [B, 512, H/8, W/8]\n",
        "        x5 = self.down4(x4)    # [B, feature_dim, H/16, W/16]\n",
        "\n",
        "        # Bottleneck with transformer\n",
        "        x5 = self.transformer_input_proj(x5)\n",
        "        B, C, H, W = x5.shape\n",
        "        x5_flat = x5.view(B, C, H * W).permute(2, 0, 1)  # [seq_len, B, C]\n",
        "        x5_flat = self.pos_encoder(x5_flat)\n",
        "        x5_trans = self.transformer(x5_flat)\n",
        "        x5 = x5_trans.permute(1, 2, 0).view(B, C, H, W)\n",
        "\n",
        "        # Decoder with attention-enhanced skip connections\n",
        "        x = self.up1(x5, x4)   # Combines bottleneck and encoder feature x4\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        return self.outc(x)\n",
        "\n",
        "###########################################\n",
        "# Evaluation Metrics: IoU and Dice Coefficient\n",
        "###########################################\n",
        "def compute_iou(pred, target, smooth=1e-6):\n",
        "    intersection = (pred & target).float().sum((1, 2))\n",
        "    union = (pred | target).float().sum((1, 2))\n",
        "    iou = (intersection + smooth) / (union + smooth)\n",
        "    return iou.mean().item()\n",
        "\n",
        "def compute_dice(pred, target, smooth=1e-6):\n",
        "    intersection = (pred * target).float().sum((1, 2))\n",
        "    dice = (2 * intersection + smooth) / (pred.float().sum((1, 2)) + target.float().sum((1, 2)) + smooth)\n",
        "    return dice.mean().item()\n",
        "\n",
        "###########################################\n",
        "# Training and Evaluation Functions\n",
        "###########################################\n",
        "def train_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for images, masks in loader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)  # [B, n_classes, H, W]\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    total_iou = 0\n",
        "    total_dice = 0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            outputs = model(images)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            total_iou += compute_iou(preds, masks)\n",
        "            total_dice += compute_dice(preds, masks)\n",
        "    return total_iou / len(loader), total_dice / len(loader)\n",
        "\n",
        "###########################################\n",
        "# Main Training Routine\n",
        "###########################################\n",
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    img_dir = \"/content/jpg_images\"  # Folder with JPG images\n",
        "    mask_dir = \"/content/mask_imgs\"  # Folder with corresponding _mask.png files\n",
        "\n",
        "    # Define transforms: resize images and masks to 256x256\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    dataset = LayoutDataset(img_dir, mask_dir, image_transform=transform, mask_transform=transform)\n",
        "    train_len = int(0.8 * len(dataset))\n",
        "    train_set, val_set = random_split(dataset, [train_len, len(dataset) - train_len])\n",
        "    train_loader = DataLoader(train_set, batch_size=4, shuffle=True)\n",
        "    val_loader = DataLoader(val_set, batch_size=4, shuffle=False)\n",
        "\n",
        "    # Create the advanced model\n",
        "    model = AdvancedTransUNet(n_channels=3, n_classes=2, bilinear=True, transformer_layers=2, nhead=8).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    num_epochs = 5\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        iou, dice = evaluate(model, val_loader, device)\n",
        "        print(f\"Epoch {epoch}/{num_epochs} - Loss: {train_loss:.4f} - Val IoU: {iou:.4f} - Val Dice: {dice:.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), \"advanced_transunet_model.pth\")\n",
        "    print(\"Training complete. Model saved as 'advanced_transunet_model.pth'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "u_U7hForoZ9-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d514f967-65bc-46ac-bacd-4f17eb1b390b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (94080000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Loss: 0.2111 - Val IoU: 0.8982 - Val Dice: 0.9444\n",
            "Epoch 2/5 - Loss: 0.0912 - Val IoU: 0.8983 - Val Dice: 0.9445\n",
            "Epoch 3/5 - Loss: 0.0807 - Val IoU: 0.9325 - Val Dice: 0.9645\n",
            "Epoch 4/5 - Loss: 0.0640 - Val IoU: 0.9548 - Val Dice: 0.9766\n",
            "Epoch 5/5 - Loss: 0.0748 - Val IoU: 0.9666 - Val Dice: 0.9827\n",
            "Training complete. Model saved as 'advanced_transunet_model.pth'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##STEP 2"
      ],
      "metadata": {
        "id": "pGYDrzBPpFmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "id": "v23ulPTIpeVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "excel file\n"
      ],
      "metadata": {
        "id": "fv2SVQsN9wR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rm -rf /content/pdf"
      ],
      "metadata": {
        "id": "PSN-bl1zcsm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Converting a Word Document to OCR Images and Labels"
      ],
      "metadata": {
        "id": "2qVKbZRx94dU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "id": "7IBdSEB4R334"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from docx import Document\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "def convert_multiple_docs_to_ocr(docx_dir, output_image_dir, output_text_dir):\n",
        "    \"\"\"\n",
        "    Extracts text from multiple Word documents, processes each page separately,\n",
        "    and saves the output as text files and images.\n",
        "\n",
        "    Parameters:\n",
        "      docx_dir (str): Directory containing Word (.docx) files.\n",
        "      output_image_dir (str): Directory to save the rendered images.\n",
        "      output_text_dir (str): Directory to save the extracted text files.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_image_dir, exist_ok=True)\n",
        "    os.makedirs(output_text_dir, exist_ok=True)\n",
        "\n",
        "    # Iterate through all .docx files in the directory\n",
        "    for file in os.listdir(docx_dir):\n",
        "        if file.endswith(\".docx\"):\n",
        "            word_path = os.path.join(docx_dir, file)\n",
        "            doc = Document(word_path)\n",
        "            base_filename = os.path.splitext(file)[0]\n",
        "\n",
        "            for i, para in enumerate(doc.paragraphs):\n",
        "                page_text = para.text.strip()\n",
        "                if not page_text:\n",
        "                    continue  # Skip empty pages\n",
        "\n",
        "                # Save extracted text\n",
        "                text_filename = f\"{base_filename}_page_{i+1}.txt\"\n",
        "                text_path = os.path.join(output_text_dir, text_filename)\n",
        "                with open(text_path, 'w', encoding='utf-8') as f:\n",
        "                    f.write(page_text)\n",
        "\n",
        "                # Render text to image\n",
        "                font = ImageFont.load_default()\n",
        "                lines = page_text.splitlines() or [\" \"]\n",
        "                line_height = font.getbbox('A')[3] - font.getbbox('A')[1] + 2\n",
        "                max_line_width = max([font.getbbox(line)[2] - font.getbbox(line)[0] for line in lines])\n",
        "                img_width = max_line_width + 20\n",
        "                img_height = line_height * len(lines) + 20\n",
        "                image = Image.new('L', (img_width, img_height), color=255)\n",
        "                draw = ImageDraw.Draw(image)\n",
        "\n",
        "                y_text = 10\n",
        "                for line in lines:\n",
        "                    draw.text((10, y_text), line, fill=0, font=font)\n",
        "                    y_text += line_height\n",
        "\n",
        "                # Save image\n",
        "                image_filename = f\"{base_filename}_page_{i+1}.jpg\"\n",
        "                image_path = os.path.join(output_image_dir, image_filename)\n",
        "                image.save(image_path)\n",
        "\n",
        "                print(f\"Processed {file}, Page {i+1}: Saved text to {text_path} and image to {image_path}.\")\n",
        "\n",
        "# Example usage:\n",
        "docx_directory = \"/content/word\"\n",
        "image_output_directory = \"ocr_images\"\n",
        "text_output_directory = \"ocr_texts\"\n",
        "convert_multiple_docs_to_ocr(docx_directory, image_output_directory, text_output_directory)\n"
      ],
      "metadata": {
        "id": "oNKJkr2X9-fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Specific Task II\n"
      ],
      "metadata": {
        "id": "0yxhp7cyGsH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "import editdistance  # pip install editdistance\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----- Character Set & Label Conversion Utilities -----\n",
        "# We use a character set covering lowercase letters (with accented characters) and digits.\n",
        "# (Note: In this version we include a space token as the last character.)\n",
        "CHAR_SET = \"abcdefghijklmnopqrstuvwxyzáéíóúüñ0123456789 \"\n",
        "# For CTC, index 0 is reserved for the blank token.\n",
        "char_to_idx = {char: idx + 1 for idx, char in enumerate(CHAR_SET)}\n",
        "idx_to_char = {idx + 1: char for idx, char in enumerate(CHAR_SET)}\n",
        "\n",
        "def text_to_labels(text):\n",
        "    \"\"\"Convert a string to a list of label indices.\"\"\"\n",
        "    return [char_to_idx[char] for char in text.lower() if char in char_to_idx]\n",
        "\n",
        "def labels_to_text(labels):\n",
        "    \"\"\"Convert a list of label indices to a string.\"\"\"\n",
        "    return \"\".join([idx_to_char[label] for label in labels if label in idx_to_char])\n",
        "\n",
        "# ----- Custom Dataset -----\n",
        "class OCRDataset(data.Dataset):\n",
        "    def __init__(self, image_dir, text_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_dir (str): Directory containing OCR images (.jpg).\n",
        "            text_dir (str): Directory containing corresponding text files (.txt).\n",
        "            transform: Optional torchvision transforms to apply.\n",
        "        \"\"\"\n",
        "        self.image_dir = image_dir\n",
        "        self.text_dir = text_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # List image and text files (case-insensitive).\n",
        "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.lower().endswith('.jpg')])\n",
        "        self.text_files = sorted([f for f in os.listdir(text_dir) if f.lower().endswith('.txt')])\n",
        "\n",
        "        print(\"Found image files:\", self.image_files)\n",
        "        print(\"Found text files:\", self.text_files)\n",
        "\n",
        "        if len(self.image_files) == 0 or len(self.text_files) == 0:\n",
        "            raise ValueError(\"No files found in the given directories.\")\n",
        "        assert len(self.image_files) == len(self.text_files), \"Mismatch between images and text files.\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image and convert to grayscale.\n",
        "        img_file = self.image_files[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_file)\n",
        "        image = Image.open(img_path).convert('L')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        else:\n",
        "            image = transforms.ToTensor()(image)\n",
        "\n",
        "        # Load the corresponding ground truth text.\n",
        "        text_file = self.text_files[idx]\n",
        "        text_path = os.path.join(self.text_dir, text_file)\n",
        "        with open(text_path, 'r', encoding='utf-8') as f:\n",
        "            gt_text = f.read().strip()\n",
        "        label = torch.tensor(text_to_labels(gt_text), dtype=torch.long)\n",
        "\n",
        "        return image, gt_text, label, len(label), img_file\n",
        "\n",
        "# ----- Hybrid CNN + Transformer Model -----\n",
        "class HybridOCRTransformer(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        \"\"\"\n",
        "        A hybrid model using a CNN backbone to extract spatial features followed by a Transformer encoder\n",
        "        to model the sequence. This architecture is inspired by recent research on transformer-based OCR\n",
        "        for historical texts.\n",
        "        Expects input images of shape (batch, 1, 32, 128).\n",
        "\n",
        "        Args:\n",
        "            num_classes (int): Number of output classes (len(CHAR_SET) + 1 for blank).\n",
        "        \"\"\"\n",
        "        super(HybridOCRTransformer, self).__init__()\n",
        "        # CNN Backbone: extract spatial features\n",
        "        self.cnn_backbone = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, padding=1),  # (32,128) -> (32,128)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),                         # (32,128) -> (16,64)\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1), # (16,64) -> (16,64)\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),                         # (16,64) -> (8,32)\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),# (8,32) -> (8,32)\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2)                          # (8,32) -> (4,16)\n",
        "        )\n",
        "        # After the CNN, feature map shape: (batch, 256, 4, 16)\n",
        "        # Flatten spatial dimensions to obtain a sequence of length 4*16 = 64, with feature dimension 256.\n",
        "        self.sequence_length = 4 * 16  # 64\n",
        "\n",
        "        # Transformer Encoder for sequence modeling.\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=8, dropout=0.1)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)\n",
        "\n",
        "        # Final classification layer on each time step.\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, 1, 32, 128)\n",
        "        features = self.cnn_backbone(x)  # -> (batch, 256, 4, 16)\n",
        "        batch, C, H, W = features.size()\n",
        "        # Flatten spatial dimensions: create sequence of shape (batch, H*W, C)\n",
        "        features = features.view(batch, C, H * W).permute(0, 2, 1)  # (batch, 64, 256)\n",
        "        # Transformer expects (sequence_length, batch, d_model)\n",
        "        features = features.permute(1, 0, 2)  # (64, batch, 256)\n",
        "        encoded = self.transformer_encoder(features)  # (64, batch, 256)\n",
        "        encoded = encoded.permute(1, 0, 2)  # (batch, 64, 256)\n",
        "        logits = self.fc(encoded)  # (batch, 64, num_classes)\n",
        "        return logits\n",
        "\n",
        "# ----- Greedy Decoder for CTC Outputs -----\n",
        "def greedy_decoder(output, blank=0):\n",
        "    \"\"\"\n",
        "    Decodes the raw output logits of shape (T, B, num_classes)\n",
        "    into predicted text strings for each batch element.\n",
        "    \"\"\"\n",
        "    # Compute argmax over classes at each time step.\n",
        "    arg_maxes = torch.argmax(output, dim=2)  # (T, B)\n",
        "    decoded_preds = []\n",
        "    for args in arg_maxes.transpose(0, 1):  # iterate over batch\n",
        "        pred = []\n",
        "        prev = blank\n",
        "        for idx in args:\n",
        "            idx = idx.item()\n",
        "            if idx != prev and idx != blank:\n",
        "                pred.append(idx)\n",
        "            prev = idx\n",
        "        decoded_preds.append(labels_to_text(pred))\n",
        "    return decoded_preds\n",
        "\n",
        "# ----- Collate Function for DataLoader -----\n",
        "def ocr_collate_fn(batch):\n",
        "    images, texts, labels, label_lengths, filenames = zip(*batch)\n",
        "    images = torch.stack(images, 0)\n",
        "    label_lengths = torch.tensor(label_lengths, dtype=torch.long)\n",
        "    labels_concat = torch.cat(labels)\n",
        "    return images, texts, labels_concat, label_lengths, filenames\n",
        "\n",
        "# ----- Evaluation Function for CER and WER -----\n",
        "def evaluate_samples_and_plot(model, eval_loader, device):\n",
        "    \"\"\"\n",
        "    Evaluates the OCR model on individual samples, computing the Character Error Rate (CER)\n",
        "    and Word Error Rate (WER) for each sample, then plots the distribution of errors.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    cer_list = []\n",
        "    wer_list = []\n",
        "    with torch.no_grad():\n",
        "        for images, texts, _, _, filenames in eval_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)  # (batch, sequence, num_classes)\n",
        "            outputs = outputs.permute(1, 0, 2)  # (sequence, batch, num_classes)\n",
        "            preds = greedy_decoder(outputs)\n",
        "            pred = preds[0]\n",
        "            gt_text = texts[0]\n",
        "            # Compute CER: edit distance normalized by length of ground truth.\n",
        "            cer = editdistance.eval(pred, gt_text) / len(gt_text) if len(gt_text) > 0 else 0\n",
        "            # Compute WER: tokenize texts by space and compute edit distance.\n",
        "            pred_words = pred.split()\n",
        "            gt_words = gt_text.split()\n",
        "            wer = editdistance.eval(pred_words, gt_words) / len(gt_words) if len(gt_words) > 0 else 0\n",
        "            cer_list.append(cer)\n",
        "            wer_list.append(wer)\n",
        "            print(f\"Filename: {filenames[0]}\")\n",
        "            print(f\"Ground Truth: {gt_text}\")\n",
        "            print(f\"Predicted   : {pred}\")\n",
        "            print(f\"CER: {cer:.3f}, WER: {wer:.3f}\\n\")\n",
        "\n",
        "    avg_cer = sum(cer_list) / len(cer_list)\n",
        "    avg_wer = sum(wer_list) / len(wer_list)\n",
        "    print(f\"Average CER: {avg_cer:.3f}\")\n",
        "    print(f\"Average WER: {avg_wer:.3f}\")\n",
        "\n",
        "    # Plot histogram of CER and WER\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(cer_list, bins=10, color=\"skyblue\", edgecolor=\"black\")\n",
        "    plt.title(\"CER Distribution\")\n",
        "    plt.xlabel(\"CER\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(wer_list, bins=10, color=\"salmon\", edgecolor=\"black\")\n",
        "    plt.title(\"WER Distribution\")\n",
        "    plt.xlabel(\"WER\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ----- Main Training and Evaluation Routine -----\n",
        "def train_and_evaluate(image_dir, text_dir, img_height=32, img_width=128,\n",
        "                       num_epochs=100, batch_size=8, learning_rate=0.0005):\n",
        "    \"\"\"\n",
        "    This routine trains the hybrid CNN+Transformer model using CTC loss.\n",
        "    After training, it computes the character error rate (CER) and word error rate (WER)\n",
        "    for each sample and then plots the results.\n",
        "    \"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((img_height, img_width)),\n",
        "        transforms.ToTensor(),  # scales pixel values to [0,1]\n",
        "    ])\n",
        "\n",
        "    dataset = OCRDataset(image_dir, text_dir, transform=transform)\n",
        "    train_loader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=ocr_collate_fn)\n",
        "\n",
        "    num_classes = len(CHAR_SET) + 1  # +1 for the blank token\n",
        "    model = HybridOCRTransformer(num_classes=num_classes)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    # CTC Loss expects raw logits.\n",
        "    ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    print(\"Starting training of the hybrid CNN+Transformer model...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        for batch_idx, (images, _, labels, label_lengths, _) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)  # (batch, sequence, num_classes)\n",
        "            outputs = outputs.permute(1, 0, 2)  # (sequence, batch, num_classes)\n",
        "            batch_size_actual = images.size(0)\n",
        "            input_lengths = torch.full(size=(batch_size_actual,), fill_value=outputs.size(0), dtype=torch.long)\n",
        "\n",
        "            loss = ctc_loss(outputs, labels, input_lengths, label_lengths)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            if (batch_idx + 1) % 10 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] Average Loss: {epoch_loss/len(train_loader):.4f}\\n\")\n",
        "\n",
        "    torch.save(model.state_dict(), \"hybrid_ocr_transformer.pth\")\n",
        "    print(\"Training complete and model saved as 'hybrid_ocr_transformer.pth'.\\n\")\n",
        "\n",
        "    # Evaluate the model on individual samples using CER/WER and plot results.\n",
        "    eval_loader = data.DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=ocr_collate_fn)\n",
        "    print(\"Evaluating model on individual samples (CER & WER):\")\n",
        "    evaluate_samples_and_plot(model, eval_loader, device)\n",
        "\n",
        "# ----- Example Usage -----\n",
        "if __name__ == \"__main__\":\n",
        "    # Set these directories to point to your OCR images (JPG) and ground truth texts (TXT).\n",
        "    image_directory = \"/content/ocr_images\"  # Directory containing .jpg files.\n",
        "    text_directory = \"/content/ocr_texts\"      # Directory containing corresponding .txt files.\n",
        "\n",
        "    # Adjust hyperparameters as needed. Here we use 100 epochs for demonstration.\n",
        "    train_and_evaluate(image_directory, text_directory, img_height=32, img_width=128,\n",
        "                       num_epochs=100, batch_size=8, learning_rate=0.0005)\n"
      ],
      "metadata": {
        "id": "xsF2ZT4mcsdx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "77e0c01f-e655-48e6-c6fc-5f5d75944ad7"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found image files: ['Buendia transcription_page_1.jpg', 'Buendia transcription_page_10.jpg', 'Buendia transcription_page_2.jpg', 'Buendia transcription_page_3.jpg', 'Buendia transcription_page_4.jpg', 'Buendia transcription_page_5.jpg', 'Buendia transcription_page_6.jpg', 'Buendia transcription_page_7.jpg', 'Buendia transcription_page_8.jpg', 'Buendia transcription_page_9.jpg', 'Constituciones sinodales transcription_page_1.jpg', 'Constituciones sinodales transcription_page_10.jpg', 'Constituciones sinodales transcription_page_11.jpg', 'Constituciones sinodales transcription_page_2.jpg', 'Constituciones sinodales transcription_page_3.jpg', 'Constituciones sinodales transcription_page_4.jpg', 'Constituciones sinodales transcription_page_5.jpg', 'Constituciones sinodales transcription_page_6.jpg', 'Constituciones sinodales transcription_page_7.jpg', 'Constituciones sinodales transcription_page_8.jpg', 'Constituciones sinodales transcription_page_9.jpg', 'Ezcaray transcription_page_1.jpg', 'Ezcaray transcription_page_2.jpg', 'Ezcaray transcription_page_3.jpg', 'Ezcaray transcription_page_4.jpg', 'Ezcaray transcription_page_5.jpg', 'Ezcaray transcription_page_6.jpg', 'Ezcaray transcription_page_7.jpg', 'Ezcaray transcription_page_8.jpg', 'Mendo transcription_page_1.jpg', 'Mendo transcription_page_10.jpg', 'Mendo transcription_page_100.jpg', 'Mendo transcription_page_101.jpg', 'Mendo transcription_page_102.jpg', 'Mendo transcription_page_103.jpg', 'Mendo transcription_page_104.jpg', 'Mendo transcription_page_105.jpg', 'Mendo transcription_page_106.jpg', 'Mendo transcription_page_107.jpg', 'Mendo transcription_page_108.jpg', 'Mendo transcription_page_109.jpg', 'Mendo transcription_page_11.jpg', 'Mendo transcription_page_110.jpg', 'Mendo transcription_page_111.jpg', 'Mendo transcription_page_112.jpg', 'Mendo transcription_page_113.jpg', 'Mendo transcription_page_114.jpg', 'Mendo transcription_page_116.jpg', 'Mendo transcription_page_117.jpg', 'Mendo transcription_page_118.jpg', 'Mendo transcription_page_119.jpg', 'Mendo transcription_page_120.jpg', 'Mendo transcription_page_121.jpg', 'Mendo transcription_page_122.jpg', 'Mendo transcription_page_124.jpg', 'Mendo transcription_page_125.jpg', 'Mendo transcription_page_126.jpg', 'Mendo transcription_page_127.jpg', 'Mendo transcription_page_128.jpg', 'Mendo transcription_page_129.jpg', 'Mendo transcription_page_13.jpg', 'Mendo transcription_page_130.jpg', 'Mendo transcription_page_131.jpg', 'Mendo transcription_page_132.jpg', 'Mendo transcription_page_133.jpg', 'Mendo transcription_page_134.jpg', 'Mendo transcription_page_135.jpg', 'Mendo transcription_page_136.jpg', 'Mendo transcription_page_137.jpg', 'Mendo transcription_page_138.jpg', 'Mendo transcription_page_139.jpg', 'Mendo transcription_page_14.jpg', 'Mendo transcription_page_140.jpg', 'Mendo transcription_page_142.jpg', 'Mendo transcription_page_143.jpg', 'Mendo transcription_page_144.jpg', 'Mendo transcription_page_145.jpg', 'Mendo transcription_page_146.jpg', 'Mendo transcription_page_147.jpg', 'Mendo transcription_page_148.jpg', 'Mendo transcription_page_149.jpg', 'Mendo transcription_page_15.jpg', 'Mendo transcription_page_150.jpg', 'Mendo transcription_page_151.jpg', 'Mendo transcription_page_152.jpg', 'Mendo transcription_page_153.jpg', 'Mendo transcription_page_154.jpg', 'Mendo transcription_page_155.jpg', 'Mendo transcription_page_156.jpg', 'Mendo transcription_page_16.jpg', 'Mendo transcription_page_17.jpg', 'Mendo transcription_page_18.jpg', 'Mendo transcription_page_19.jpg', 'Mendo transcription_page_2.jpg', 'Mendo transcription_page_20.jpg', 'Mendo transcription_page_21.jpg', 'Mendo transcription_page_22.jpg', 'Mendo transcription_page_23.jpg', 'Mendo transcription_page_24.jpg', 'Mendo transcription_page_26.jpg', 'Mendo transcription_page_27.jpg', 'Mendo transcription_page_28.jpg', 'Mendo transcription_page_29.jpg', 'Mendo transcription_page_3.jpg', 'Mendo transcription_page_30.jpg', 'Mendo transcription_page_31.jpg', 'Mendo transcription_page_32.jpg', 'Mendo transcription_page_33.jpg', 'Mendo transcription_page_34.jpg', 'Mendo transcription_page_35.jpg', 'Mendo transcription_page_37.jpg', 'Mendo transcription_page_38.jpg', 'Mendo transcription_page_39.jpg', 'Mendo transcription_page_4.jpg', 'Mendo transcription_page_41.jpg', 'Mendo transcription_page_42.jpg', 'Mendo transcription_page_43.jpg', 'Mendo transcription_page_44.jpg', 'Mendo transcription_page_45.jpg', 'Mendo transcription_page_46.jpg', 'Mendo transcription_page_48.jpg', 'Mendo transcription_page_49.jpg', 'Mendo transcription_page_5.jpg', 'Mendo transcription_page_50.jpg', 'Mendo transcription_page_51.jpg', 'Mendo transcription_page_52.jpg', 'Mendo transcription_page_53.jpg', 'Mendo transcription_page_54.jpg', 'Mendo transcription_page_55.jpg', 'Mendo transcription_page_56.jpg', 'Mendo transcription_page_57.jpg', 'Mendo transcription_page_58.jpg', 'Mendo transcription_page_59.jpg', 'Mendo transcription_page_6.jpg', 'Mendo transcription_page_60.jpg', 'Mendo transcription_page_61.jpg', 'Mendo transcription_page_62.jpg', 'Mendo transcription_page_63.jpg', 'Mendo transcription_page_64.jpg', 'Mendo transcription_page_65.jpg', 'Mendo transcription_page_66.jpg', 'Mendo transcription_page_67.jpg', 'Mendo transcription_page_68.jpg', 'Mendo transcription_page_69.jpg', 'Mendo transcription_page_7.jpg', 'Mendo transcription_page_70.jpg', 'Mendo transcription_page_71.jpg', 'Mendo transcription_page_72.jpg', 'Mendo transcription_page_73.jpg', 'Mendo transcription_page_74.jpg', 'Mendo transcription_page_75.jpg', 'Mendo transcription_page_77.jpg', 'Mendo transcription_page_78.jpg', 'Mendo transcription_page_79.jpg', 'Mendo transcription_page_8.jpg', 'Mendo transcription_page_80.jpg', 'Mendo transcription_page_81.jpg', 'Mendo transcription_page_82.jpg', 'Mendo transcription_page_83.jpg', 'Mendo transcription_page_84.jpg', 'Mendo transcription_page_85.jpg', 'Mendo transcription_page_86.jpg', 'Mendo transcription_page_87.jpg', 'Mendo transcription_page_88.jpg', 'Mendo transcription_page_89.jpg', 'Mendo transcription_page_9.jpg', 'Mendo transcription_page_90.jpg', 'Mendo transcription_page_91.jpg', 'Mendo transcription_page_92.jpg', 'Mendo transcription_page_93.jpg', 'Mendo transcription_page_94.jpg', 'Mendo transcription_page_95.jpg', 'Mendo transcription_page_96.jpg', 'Mendo transcription_page_97.jpg', 'Mendo transcription_page_98.jpg', 'Mendo transcription_page_99.jpg', 'PORCONES.228.35 1636 transcription_page_1.jpg', 'PORCONES.228.35 1636 transcription_page_10.jpg', 'PORCONES.228.35 1636 transcription_page_2.jpg', 'PORCONES.228.35 1636 transcription_page_3.jpg', 'PORCONES.228.35 1636 transcription_page_4.jpg', 'PORCONES.228.35 1636 transcription_page_5.jpg', 'PORCONES.228.35 1636 transcription_page_6.jpg', 'PORCONES.228.35 1636 transcription_page_7.jpg', 'PORCONES.228.35 1636 transcription_page_8.jpg', 'PORCONES.228.35 1636 transcription_page_9.jpg', 'Paredes transcription_page_1.jpg', 'Paredes transcription_page_100.jpg', 'Paredes transcription_page_101.jpg', 'Paredes transcription_page_102.jpg', 'Paredes transcription_page_103.jpg', 'Paredes transcription_page_104.jpg', 'Paredes transcription_page_105.jpg', 'Paredes transcription_page_106.jpg', 'Paredes transcription_page_13.jpg', 'Paredes transcription_page_14.jpg', 'Paredes transcription_page_15.jpg', 'Paredes transcription_page_16.jpg', 'Paredes transcription_page_17.jpg', 'Paredes transcription_page_18.jpg', 'Paredes transcription_page_19.jpg', 'Paredes transcription_page_20.jpg', 'Paredes transcription_page_21.jpg', 'Paredes transcription_page_22.jpg', 'Paredes transcription_page_23.jpg', 'Paredes transcription_page_24.jpg', 'Paredes transcription_page_26.jpg', 'Paredes transcription_page_28.jpg', 'Paredes transcription_page_29.jpg', 'Paredes transcription_page_3.jpg', 'Paredes transcription_page_30.jpg', 'Paredes transcription_page_31.jpg', 'Paredes transcription_page_32.jpg', 'Paredes transcription_page_33.jpg', 'Paredes transcription_page_34.jpg', 'Paredes transcription_page_35.jpg', 'Paredes transcription_page_36.jpg', 'Paredes transcription_page_37.jpg', 'Paredes transcription_page_38.jpg', 'Paredes transcription_page_39.jpg', 'Paredes transcription_page_40.jpg', 'Paredes transcription_page_41.jpg', 'Paredes transcription_page_42.jpg', 'Paredes transcription_page_43.jpg', 'Paredes transcription_page_44.jpg', 'Paredes transcription_page_45.jpg', 'Paredes transcription_page_46.jpg', 'Paredes transcription_page_47.jpg', 'Paredes transcription_page_48.jpg', 'Paredes transcription_page_5.jpg', 'Paredes transcription_page_50.jpg', 'Paredes transcription_page_52.jpg', 'Paredes transcription_page_53.jpg', 'Paredes transcription_page_54.jpg', 'Paredes transcription_page_55.jpg', 'Paredes transcription_page_57.jpg', 'Paredes transcription_page_58.jpg', 'Paredes transcription_page_59.jpg', 'Paredes transcription_page_6.jpg', 'Paredes transcription_page_60.jpg', 'Paredes transcription_page_61.jpg', 'Paredes transcription_page_62.jpg', 'Paredes transcription_page_63.jpg', 'Paredes transcription_page_64.jpg', 'Paredes transcription_page_65.jpg', 'Paredes transcription_page_67.jpg', 'Paredes transcription_page_69.jpg', 'Paredes transcription_page_70.jpg', 'Paredes transcription_page_71.jpg', 'Paredes transcription_page_72.jpg', 'Paredes transcription_page_73.jpg', 'Paredes transcription_page_74.jpg', 'Paredes transcription_page_75.jpg', 'Paredes transcription_page_76.jpg', 'Paredes transcription_page_77.jpg', 'Paredes transcription_page_78.jpg', 'Paredes transcription_page_79.jpg', 'Paredes transcription_page_80.jpg', 'Paredes transcription_page_81.jpg', 'Paredes transcription_page_82.jpg', 'Paredes transcription_page_83.jpg', 'Paredes transcription_page_84.jpg', 'Paredes transcription_page_85.jpg', 'Paredes transcription_page_86.jpg', 'Paredes transcription_page_87.jpg', 'Paredes transcription_page_88.jpg', 'Paredes transcription_page_89.jpg', 'Paredes transcription_page_90.jpg', 'Paredes transcription_page_91.jpg', 'Paredes transcription_page_92.jpg', 'Paredes transcription_page_93.jpg', 'Paredes transcription_page_94.jpg', 'Paredes transcription_page_95.jpg', 'Paredes transcription_page_96.jpg', 'Paredes transcription_page_97.jpg', 'Paredes transcription_page_98.jpg', 'Paredes transcription_page_99.jpg']\n",
            "Found text files: ['Buendia transcription_page_1.txt', 'Buendia transcription_page_10.txt', 'Buendia transcription_page_2.txt', 'Buendia transcription_page_3.txt', 'Buendia transcription_page_4.txt', 'Buendia transcription_page_5.txt', 'Buendia transcription_page_6.txt', 'Buendia transcription_page_7.txt', 'Buendia transcription_page_8.txt', 'Buendia transcription_page_9.txt', 'Constituciones sinodales transcription_page_1.txt', 'Constituciones sinodales transcription_page_10.txt', 'Constituciones sinodales transcription_page_11.txt', 'Constituciones sinodales transcription_page_2.txt', 'Constituciones sinodales transcription_page_3.txt', 'Constituciones sinodales transcription_page_4.txt', 'Constituciones sinodales transcription_page_5.txt', 'Constituciones sinodales transcription_page_6.txt', 'Constituciones sinodales transcription_page_7.txt', 'Constituciones sinodales transcription_page_8.txt', 'Constituciones sinodales transcription_page_9.txt', 'Ezcaray transcription_page_1.txt', 'Ezcaray transcription_page_2.txt', 'Ezcaray transcription_page_3.txt', 'Ezcaray transcription_page_4.txt', 'Ezcaray transcription_page_5.txt', 'Ezcaray transcription_page_6.txt', 'Ezcaray transcription_page_7.txt', 'Ezcaray transcription_page_8.txt', 'Mendo transcription_page_1.txt', 'Mendo transcription_page_10.txt', 'Mendo transcription_page_100.txt', 'Mendo transcription_page_101.txt', 'Mendo transcription_page_102.txt', 'Mendo transcription_page_103.txt', 'Mendo transcription_page_104.txt', 'Mendo transcription_page_105.txt', 'Mendo transcription_page_106.txt', 'Mendo transcription_page_107.txt', 'Mendo transcription_page_108.txt', 'Mendo transcription_page_109.txt', 'Mendo transcription_page_11.txt', 'Mendo transcription_page_110.txt', 'Mendo transcription_page_111.txt', 'Mendo transcription_page_112.txt', 'Mendo transcription_page_113.txt', 'Mendo transcription_page_114.txt', 'Mendo transcription_page_116.txt', 'Mendo transcription_page_117.txt', 'Mendo transcription_page_118.txt', 'Mendo transcription_page_119.txt', 'Mendo transcription_page_120.txt', 'Mendo transcription_page_121.txt', 'Mendo transcription_page_122.txt', 'Mendo transcription_page_124.txt', 'Mendo transcription_page_125.txt', 'Mendo transcription_page_126.txt', 'Mendo transcription_page_127.txt', 'Mendo transcription_page_128.txt', 'Mendo transcription_page_129.txt', 'Mendo transcription_page_13.txt', 'Mendo transcription_page_130.txt', 'Mendo transcription_page_131.txt', 'Mendo transcription_page_132.txt', 'Mendo transcription_page_133.txt', 'Mendo transcription_page_134.txt', 'Mendo transcription_page_135.txt', 'Mendo transcription_page_136.txt', 'Mendo transcription_page_137.txt', 'Mendo transcription_page_138.txt', 'Mendo transcription_page_139.txt', 'Mendo transcription_page_14.txt', 'Mendo transcription_page_140.txt', 'Mendo transcription_page_142.txt', 'Mendo transcription_page_143.txt', 'Mendo transcription_page_144.txt', 'Mendo transcription_page_145.txt', 'Mendo transcription_page_146.txt', 'Mendo transcription_page_147.txt', 'Mendo transcription_page_148.txt', 'Mendo transcription_page_149.txt', 'Mendo transcription_page_15.txt', 'Mendo transcription_page_150.txt', 'Mendo transcription_page_151.txt', 'Mendo transcription_page_152.txt', 'Mendo transcription_page_153.txt', 'Mendo transcription_page_154.txt', 'Mendo transcription_page_155.txt', 'Mendo transcription_page_156.txt', 'Mendo transcription_page_16.txt', 'Mendo transcription_page_17.txt', 'Mendo transcription_page_18.txt', 'Mendo transcription_page_19.txt', 'Mendo transcription_page_2.txt', 'Mendo transcription_page_20.txt', 'Mendo transcription_page_21.txt', 'Mendo transcription_page_22.txt', 'Mendo transcription_page_23.txt', 'Mendo transcription_page_24.txt', 'Mendo transcription_page_26.txt', 'Mendo transcription_page_27.txt', 'Mendo transcription_page_28.txt', 'Mendo transcription_page_29.txt', 'Mendo transcription_page_3.txt', 'Mendo transcription_page_30.txt', 'Mendo transcription_page_31.txt', 'Mendo transcription_page_32.txt', 'Mendo transcription_page_33.txt', 'Mendo transcription_page_34.txt', 'Mendo transcription_page_35.txt', 'Mendo transcription_page_37.txt', 'Mendo transcription_page_38.txt', 'Mendo transcription_page_39.txt', 'Mendo transcription_page_4.txt', 'Mendo transcription_page_41.txt', 'Mendo transcription_page_42.txt', 'Mendo transcription_page_43.txt', 'Mendo transcription_page_44.txt', 'Mendo transcription_page_45.txt', 'Mendo transcription_page_46.txt', 'Mendo transcription_page_48.txt', 'Mendo transcription_page_49.txt', 'Mendo transcription_page_5.txt', 'Mendo transcription_page_50.txt', 'Mendo transcription_page_51.txt', 'Mendo transcription_page_52.txt', 'Mendo transcription_page_53.txt', 'Mendo transcription_page_54.txt', 'Mendo transcription_page_55.txt', 'Mendo transcription_page_56.txt', 'Mendo transcription_page_57.txt', 'Mendo transcription_page_58.txt', 'Mendo transcription_page_59.txt', 'Mendo transcription_page_6.txt', 'Mendo transcription_page_60.txt', 'Mendo transcription_page_61.txt', 'Mendo transcription_page_62.txt', 'Mendo transcription_page_63.txt', 'Mendo transcription_page_64.txt', 'Mendo transcription_page_65.txt', 'Mendo transcription_page_66.txt', 'Mendo transcription_page_67.txt', 'Mendo transcription_page_68.txt', 'Mendo transcription_page_69.txt', 'Mendo transcription_page_7.txt', 'Mendo transcription_page_70.txt', 'Mendo transcription_page_71.txt', 'Mendo transcription_page_72.txt', 'Mendo transcription_page_73.txt', 'Mendo transcription_page_74.txt', 'Mendo transcription_page_75.txt', 'Mendo transcription_page_77.txt', 'Mendo transcription_page_78.txt', 'Mendo transcription_page_79.txt', 'Mendo transcription_page_8.txt', 'Mendo transcription_page_80.txt', 'Mendo transcription_page_81.txt', 'Mendo transcription_page_82.txt', 'Mendo transcription_page_83.txt', 'Mendo transcription_page_84.txt', 'Mendo transcription_page_85.txt', 'Mendo transcription_page_86.txt', 'Mendo transcription_page_87.txt', 'Mendo transcription_page_88.txt', 'Mendo transcription_page_89.txt', 'Mendo transcription_page_9.txt', 'Mendo transcription_page_90.txt', 'Mendo transcription_page_91.txt', 'Mendo transcription_page_92.txt', 'Mendo transcription_page_93.txt', 'Mendo transcription_page_94.txt', 'Mendo transcription_page_95.txt', 'Mendo transcription_page_96.txt', 'Mendo transcription_page_97.txt', 'Mendo transcription_page_98.txt', 'Mendo transcription_page_99.txt', 'PORCONES.228.35 1636 transcription_page_1.txt', 'PORCONES.228.35 1636 transcription_page_10.txt', 'PORCONES.228.35 1636 transcription_page_2.txt', 'PORCONES.228.35 1636 transcription_page_3.txt', 'PORCONES.228.35 1636 transcription_page_4.txt', 'PORCONES.228.35 1636 transcription_page_5.txt', 'PORCONES.228.35 1636 transcription_page_6.txt', 'PORCONES.228.35 1636 transcription_page_7.txt', 'PORCONES.228.35 1636 transcription_page_8.txt', 'PORCONES.228.35 1636 transcription_page_9.txt', 'Paredes transcription_page_1.txt', 'Paredes transcription_page_100.txt', 'Paredes transcription_page_101.txt', 'Paredes transcription_page_102.txt', 'Paredes transcription_page_103.txt', 'Paredes transcription_page_104.txt', 'Paredes transcription_page_105.txt', 'Paredes transcription_page_106.txt', 'Paredes transcription_page_13.txt', 'Paredes transcription_page_14.txt', 'Paredes transcription_page_15.txt', 'Paredes transcription_page_16.txt', 'Paredes transcription_page_17.txt', 'Paredes transcription_page_18.txt', 'Paredes transcription_page_19.txt', 'Paredes transcription_page_20.txt', 'Paredes transcription_page_21.txt', 'Paredes transcription_page_22.txt', 'Paredes transcription_page_23.txt', 'Paredes transcription_page_24.txt', 'Paredes transcription_page_26.txt', 'Paredes transcription_page_28.txt', 'Paredes transcription_page_29.txt', 'Paredes transcription_page_3.txt', 'Paredes transcription_page_30.txt', 'Paredes transcription_page_31.txt', 'Paredes transcription_page_32.txt', 'Paredes transcription_page_33.txt', 'Paredes transcription_page_34.txt', 'Paredes transcription_page_35.txt', 'Paredes transcription_page_36.txt', 'Paredes transcription_page_37.txt', 'Paredes transcription_page_38.txt', 'Paredes transcription_page_39.txt', 'Paredes transcription_page_40.txt', 'Paredes transcription_page_41.txt', 'Paredes transcription_page_42.txt', 'Paredes transcription_page_43.txt', 'Paredes transcription_page_44.txt', 'Paredes transcription_page_45.txt', 'Paredes transcription_page_46.txt', 'Paredes transcription_page_47.txt', 'Paredes transcription_page_48.txt', 'Paredes transcription_page_5.txt', 'Paredes transcription_page_50.txt', 'Paredes transcription_page_52.txt', 'Paredes transcription_page_53.txt', 'Paredes transcription_page_54.txt', 'Paredes transcription_page_55.txt', 'Paredes transcription_page_57.txt', 'Paredes transcription_page_58.txt', 'Paredes transcription_page_59.txt', 'Paredes transcription_page_6.txt', 'Paredes transcription_page_60.txt', 'Paredes transcription_page_61.txt', 'Paredes transcription_page_62.txt', 'Paredes transcription_page_63.txt', 'Paredes transcription_page_64.txt', 'Paredes transcription_page_65.txt', 'Paredes transcription_page_67.txt', 'Paredes transcription_page_69.txt', 'Paredes transcription_page_70.txt', 'Paredes transcription_page_71.txt', 'Paredes transcription_page_72.txt', 'Paredes transcription_page_73.txt', 'Paredes transcription_page_74.txt', 'Paredes transcription_page_75.txt', 'Paredes transcription_page_76.txt', 'Paredes transcription_page_77.txt', 'Paredes transcription_page_78.txt', 'Paredes transcription_page_79.txt', 'Paredes transcription_page_80.txt', 'Paredes transcription_page_81.txt', 'Paredes transcription_page_82.txt', 'Paredes transcription_page_83.txt', 'Paredes transcription_page_84.txt', 'Paredes transcription_page_85.txt', 'Paredes transcription_page_86.txt', 'Paredes transcription_page_87.txt', 'Paredes transcription_page_88.txt', 'Paredes transcription_page_89.txt', 'Paredes transcription_page_90.txt', 'Paredes transcription_page_91.txt', 'Paredes transcription_page_92.txt', 'Paredes transcription_page_93.txt', 'Paredes transcription_page_94.txt', 'Paredes transcription_page_95.txt', 'Paredes transcription_page_96.txt', 'Paredes transcription_page_97.txt', 'Paredes transcription_page_98.txt', 'Paredes transcription_page_99.txt']\n",
            "Starting training of the hybrid CNN+Transformer model...\n",
            "Epoch [1/100], Batch [10/35], Loss: 0.1799\n",
            "Epoch [1/100], Batch [20/35], Loss: 1.6263\n",
            "Epoch [1/100], Batch [30/35], Loss: 2.0198\n",
            "Epoch [1/100] Average Loss: 1.6298\n",
            "\n",
            "Epoch [2/100], Batch [10/35], Loss: 2.9774\n",
            "Epoch [2/100], Batch [20/35], Loss: 3.3396\n",
            "Epoch [2/100], Batch [30/35], Loss: 2.4633\n",
            "Epoch [2/100] Average Loss: 3.0397\n",
            "\n",
            "Epoch [3/100], Batch [10/35], Loss: 2.2774\n",
            "Epoch [3/100], Batch [20/35], Loss: 2.4836\n",
            "Epoch [3/100], Batch [30/35], Loss: 2.8296\n",
            "Epoch [3/100] Average Loss: 2.9379\n",
            "\n",
            "Epoch [4/100], Batch [10/35], Loss: 2.4246\n",
            "Epoch [4/100], Batch [20/35], Loss: 3.3537\n",
            "Epoch [4/100], Batch [30/35], Loss: 2.3453\n",
            "Epoch [4/100] Average Loss: 3.1100\n",
            "\n",
            "Epoch [5/100], Batch [10/35], Loss: 3.1978\n",
            "Epoch [5/100], Batch [20/35], Loss: 2.1359\n",
            "Epoch [5/100], Batch [30/35], Loss: 2.6841\n",
            "Epoch [5/100] Average Loss: 2.8615\n",
            "\n",
            "Epoch [6/100], Batch [10/35], Loss: 2.2379\n",
            "Epoch [6/100], Batch [20/35], Loss: 2.5702\n",
            "Epoch [6/100], Batch [30/35], Loss: 2.5493\n",
            "Epoch [6/100] Average Loss: 2.5344\n",
            "\n",
            "Epoch [7/100], Batch [10/35], Loss: 2.5567\n",
            "Epoch [7/100], Batch [20/35], Loss: 3.0468\n",
            "Epoch [7/100], Batch [30/35], Loss: 2.2551\n",
            "Epoch [7/100] Average Loss: 2.6232\n",
            "\n",
            "Epoch [8/100], Batch [10/35], Loss: 2.5492\n",
            "Epoch [8/100], Batch [20/35], Loss: 2.5873\n",
            "Epoch [8/100], Batch [30/35], Loss: 2.1958\n",
            "Epoch [8/100] Average Loss: 2.4491\n",
            "\n",
            "Epoch [9/100], Batch [10/35], Loss: 1.9093\n",
            "Epoch [9/100], Batch [20/35], Loss: 2.9681\n",
            "Epoch [9/100], Batch [30/35], Loss: 2.7281\n",
            "Epoch [9/100] Average Loss: 2.4632\n",
            "\n",
            "Epoch [10/100], Batch [10/35], Loss: 2.5145\n",
            "Epoch [10/100], Batch [20/35], Loss: 2.6268\n",
            "Epoch [10/100], Batch [30/35], Loss: 2.4250\n",
            "Epoch [10/100] Average Loss: 2.4465\n",
            "\n",
            "Epoch [11/100], Batch [10/35], Loss: 2.0188\n",
            "Epoch [11/100], Batch [20/35], Loss: 2.1841\n",
            "Epoch [11/100], Batch [30/35], Loss: 2.5907\n",
            "Epoch [11/100] Average Loss: 2.4323\n",
            "\n",
            "Epoch [12/100], Batch [10/35], Loss: 2.7620\n",
            "Epoch [12/100], Batch [20/35], Loss: 3.1219\n",
            "Epoch [12/100], Batch [30/35], Loss: 2.6529\n",
            "Epoch [12/100] Average Loss: 2.4328\n",
            "\n",
            "Epoch [13/100], Batch [10/35], Loss: 2.5844\n",
            "Epoch [13/100], Batch [20/35], Loss: 2.3419\n",
            "Epoch [13/100], Batch [30/35], Loss: 1.7759\n",
            "Epoch [13/100] Average Loss: 2.4151\n",
            "\n",
            "Epoch [14/100], Batch [10/35], Loss: 2.8982\n",
            "Epoch [14/100], Batch [20/35], Loss: 2.8283\n",
            "Epoch [14/100], Batch [30/35], Loss: 2.7689\n",
            "Epoch [14/100] Average Loss: 2.4127\n",
            "\n",
            "Epoch [15/100], Batch [10/35], Loss: 2.1360\n",
            "Epoch [15/100], Batch [20/35], Loss: 2.4190\n",
            "Epoch [15/100], Batch [30/35], Loss: 2.5056\n",
            "Epoch [15/100] Average Loss: 2.4304\n",
            "\n",
            "Epoch [16/100], Batch [10/35], Loss: 2.8415\n",
            "Epoch [16/100], Batch [20/35], Loss: 1.8597\n",
            "Epoch [16/100], Batch [30/35], Loss: 2.4829\n",
            "Epoch [16/100] Average Loss: 2.4015\n",
            "\n",
            "Epoch [17/100], Batch [10/35], Loss: 2.8349\n",
            "Epoch [17/100], Batch [20/35], Loss: 2.4400\n",
            "Epoch [17/100], Batch [30/35], Loss: 2.0988\n",
            "Epoch [17/100] Average Loss: 2.4237\n",
            "\n",
            "Epoch [18/100], Batch [10/35], Loss: 2.5251\n",
            "Epoch [18/100], Batch [20/35], Loss: 2.4266\n",
            "Epoch [18/100], Batch [30/35], Loss: 2.4831\n",
            "Epoch [18/100] Average Loss: 2.3747\n",
            "\n",
            "Epoch [19/100], Batch [10/35], Loss: 1.1000\n",
            "Epoch [19/100], Batch [20/35], Loss: 1.5337\n",
            "Epoch [19/100], Batch [30/35], Loss: 2.6519\n",
            "Epoch [19/100] Average Loss: 2.4459\n",
            "\n",
            "Epoch [20/100], Batch [10/35], Loss: 2.9433\n",
            "Epoch [20/100], Batch [20/35], Loss: 2.1438\n",
            "Epoch [20/100], Batch [30/35], Loss: 2.8125\n",
            "Epoch [20/100] Average Loss: 2.4098\n",
            "\n",
            "Epoch [21/100], Batch [10/35], Loss: 2.4310\n",
            "Epoch [21/100], Batch [20/35], Loss: 2.8493\n",
            "Epoch [21/100], Batch [30/35], Loss: 2.5762\n",
            "Epoch [21/100] Average Loss: 2.3905\n",
            "\n",
            "Epoch [22/100], Batch [10/35], Loss: 2.8028\n",
            "Epoch [22/100], Batch [20/35], Loss: 2.5607\n",
            "Epoch [22/100], Batch [30/35], Loss: 2.0418\n",
            "Epoch [22/100] Average Loss: 2.3695\n",
            "\n",
            "Epoch [23/100], Batch [10/35], Loss: 1.7457\n",
            "Epoch [23/100], Batch [20/35], Loss: 1.3867\n",
            "Epoch [23/100], Batch [30/35], Loss: 2.5874\n",
            "Epoch [23/100] Average Loss: 2.3695\n",
            "\n",
            "Epoch [24/100], Batch [10/35], Loss: 1.7330\n",
            "Epoch [24/100], Batch [20/35], Loss: 2.3780\n",
            "Epoch [24/100], Batch [30/35], Loss: 1.7923\n",
            "Epoch [24/100] Average Loss: 2.3742\n",
            "\n",
            "Epoch [25/100], Batch [10/35], Loss: 2.5208\n",
            "Epoch [25/100], Batch [20/35], Loss: 2.6193\n",
            "Epoch [25/100], Batch [30/35], Loss: 2.8350\n",
            "Epoch [25/100] Average Loss: 2.3596\n",
            "\n",
            "Epoch [26/100], Batch [10/35], Loss: 2.1889\n",
            "Epoch [26/100], Batch [20/35], Loss: 2.0863\n",
            "Epoch [26/100], Batch [30/35], Loss: 2.2337\n",
            "Epoch [26/100] Average Loss: 2.3550\n",
            "\n",
            "Epoch [27/100], Batch [10/35], Loss: 2.2715\n",
            "Epoch [27/100], Batch [20/35], Loss: 2.8732\n",
            "Epoch [27/100], Batch [30/35], Loss: 1.7418\n",
            "Epoch [27/100] Average Loss: 2.3288\n",
            "\n",
            "Epoch [28/100], Batch [10/35], Loss: 2.5241\n",
            "Epoch [28/100], Batch [20/35], Loss: 2.4396\n",
            "Epoch [28/100], Batch [30/35], Loss: 1.4207\n",
            "Epoch [28/100] Average Loss: 2.3225\n",
            "\n",
            "Epoch [29/100], Batch [10/35], Loss: 2.7292\n",
            "Epoch [29/100], Batch [20/35], Loss: 2.4506\n",
            "Epoch [29/100], Batch [30/35], Loss: 2.4919\n",
            "Epoch [29/100] Average Loss: 2.3126\n",
            "\n",
            "Epoch [30/100], Batch [10/35], Loss: 2.6506\n",
            "Epoch [30/100], Batch [20/35], Loss: 2.0805\n",
            "Epoch [30/100], Batch [30/35], Loss: 2.8743\n",
            "Epoch [30/100] Average Loss: 2.3030\n",
            "\n",
            "Epoch [31/100], Batch [10/35], Loss: 2.2293\n",
            "Epoch [31/100], Batch [20/35], Loss: 3.0699\n",
            "Epoch [31/100], Batch [30/35], Loss: 2.6372\n",
            "Epoch [31/100] Average Loss: 2.3970\n",
            "\n",
            "Epoch [32/100], Batch [10/35], Loss: 2.4910\n",
            "Epoch [32/100], Batch [20/35], Loss: 2.3959\n",
            "Epoch [32/100], Batch [30/35], Loss: 3.1355\n",
            "Epoch [32/100] Average Loss: 2.3333\n",
            "\n",
            "Epoch [33/100], Batch [10/35], Loss: 1.8290\n",
            "Epoch [33/100], Batch [20/35], Loss: 1.9102\n",
            "Epoch [33/100], Batch [30/35], Loss: 2.1548\n",
            "Epoch [33/100] Average Loss: 2.3084\n",
            "\n",
            "Epoch [34/100], Batch [10/35], Loss: 2.3388\n",
            "Epoch [34/100], Batch [20/35], Loss: 2.0224\n",
            "Epoch [34/100], Batch [30/35], Loss: 2.0900\n",
            "Epoch [34/100] Average Loss: 2.2942\n",
            "\n",
            "Epoch [35/100], Batch [10/35], Loss: 2.0328\n",
            "Epoch [35/100], Batch [20/35], Loss: 2.5249\n",
            "Epoch [35/100], Batch [30/35], Loss: 2.4261\n",
            "Epoch [35/100] Average Loss: 2.2760\n",
            "\n",
            "Epoch [36/100], Batch [10/35], Loss: 2.4388\n",
            "Epoch [36/100], Batch [20/35], Loss: 2.7913\n",
            "Epoch [36/100], Batch [30/35], Loss: 2.4713\n",
            "Epoch [36/100] Average Loss: 2.3152\n",
            "\n",
            "Epoch [37/100], Batch [10/35], Loss: 2.4234\n",
            "Epoch [37/100], Batch [20/35], Loss: 2.0759\n",
            "Epoch [37/100], Batch [30/35], Loss: 2.0648\n",
            "Epoch [37/100] Average Loss: 2.2997\n",
            "\n",
            "Epoch [38/100], Batch [10/35], Loss: 2.0227\n",
            "Epoch [38/100], Batch [20/35], Loss: 2.4356\n",
            "Epoch [38/100], Batch [30/35], Loss: 2.6821\n",
            "Epoch [38/100] Average Loss: 2.3061\n",
            "\n",
            "Epoch [39/100], Batch [10/35], Loss: 2.3203\n",
            "Epoch [39/100], Batch [20/35], Loss: 1.7263\n",
            "Epoch [39/100], Batch [30/35], Loss: 2.7205\n",
            "Epoch [39/100] Average Loss: 2.2978\n",
            "\n",
            "Epoch [40/100], Batch [10/35], Loss: 2.3534\n",
            "Epoch [40/100], Batch [20/35], Loss: 2.6078\n",
            "Epoch [40/100], Batch [30/35], Loss: 2.3106\n",
            "Epoch [40/100] Average Loss: 2.2668\n",
            "\n",
            "Epoch [41/100], Batch [10/35], Loss: 2.5647\n",
            "Epoch [41/100], Batch [20/35], Loss: 2.3554\n",
            "Epoch [41/100], Batch [30/35], Loss: 2.2189\n",
            "Epoch [41/100] Average Loss: 2.2569\n",
            "\n",
            "Epoch [42/100], Batch [10/35], Loss: 1.2518\n",
            "Epoch [42/100], Batch [20/35], Loss: 1.8594\n",
            "Epoch [42/100], Batch [30/35], Loss: 2.8202\n",
            "Epoch [42/100] Average Loss: 2.2468\n",
            "\n",
            "Epoch [43/100], Batch [10/35], Loss: 2.6198\n",
            "Epoch [43/100], Batch [20/35], Loss: 2.1206\n",
            "Epoch [43/100], Batch [30/35], Loss: 2.3313\n",
            "Epoch [43/100] Average Loss: 2.2316\n",
            "\n",
            "Epoch [44/100], Batch [10/35], Loss: 2.5374\n",
            "Epoch [44/100], Batch [20/35], Loss: 2.4252\n",
            "Epoch [44/100], Batch [30/35], Loss: 2.5159\n",
            "Epoch [44/100] Average Loss: 2.2325\n",
            "\n",
            "Epoch [45/100], Batch [10/35], Loss: 2.4006\n",
            "Epoch [45/100], Batch [20/35], Loss: 2.3483\n",
            "Epoch [45/100], Batch [30/35], Loss: 1.5772\n",
            "Epoch [45/100] Average Loss: 2.2386\n",
            "\n",
            "Epoch [46/100], Batch [10/35], Loss: 1.9430\n",
            "Epoch [46/100], Batch [20/35], Loss: 3.0026\n",
            "Epoch [46/100], Batch [30/35], Loss: 2.8287\n",
            "Epoch [46/100] Average Loss: 2.4878\n",
            "\n",
            "Epoch [47/100], Batch [10/35], Loss: 2.2688\n",
            "Epoch [47/100], Batch [20/35], Loss: 2.9728\n",
            "Epoch [47/100], Batch [30/35], Loss: 2.0536\n",
            "Epoch [47/100] Average Loss: 2.3708\n",
            "\n",
            "Epoch [48/100], Batch [10/35], Loss: 2.5291\n",
            "Epoch [48/100], Batch [20/35], Loss: 2.3041\n",
            "Epoch [48/100], Batch [30/35], Loss: 2.0554\n",
            "Epoch [48/100] Average Loss: 2.2508\n",
            "\n",
            "Epoch [49/100], Batch [10/35], Loss: 2.0800\n",
            "Epoch [49/100], Batch [20/35], Loss: 2.0573\n",
            "Epoch [49/100], Batch [30/35], Loss: 2.0984\n",
            "Epoch [49/100] Average Loss: 2.2411\n",
            "\n",
            "Epoch [50/100], Batch [10/35], Loss: 2.0922\n",
            "Epoch [50/100], Batch [20/35], Loss: 2.0011\n",
            "Epoch [50/100], Batch [30/35], Loss: 2.3950\n",
            "Epoch [50/100] Average Loss: 2.2235\n",
            "\n",
            "Epoch [51/100], Batch [10/35], Loss: 2.5110\n",
            "Epoch [51/100], Batch [20/35], Loss: 2.2376\n",
            "Epoch [51/100], Batch [30/35], Loss: 2.3735\n",
            "Epoch [51/100] Average Loss: 2.2044\n",
            "\n",
            "Epoch [52/100], Batch [10/35], Loss: 2.6892\n",
            "Epoch [52/100], Batch [20/35], Loss: 2.3547\n",
            "Epoch [52/100], Batch [30/35], Loss: 2.0971\n",
            "Epoch [52/100] Average Loss: 2.1915\n",
            "\n",
            "Epoch [53/100], Batch [10/35], Loss: 2.5509\n",
            "Epoch [53/100], Batch [20/35], Loss: 2.4012\n",
            "Epoch [53/100], Batch [30/35], Loss: 1.1436\n",
            "Epoch [53/100] Average Loss: 2.1777\n",
            "\n",
            "Epoch [54/100], Batch [10/35], Loss: 2.0886\n",
            "Epoch [54/100], Batch [20/35], Loss: 2.1960\n",
            "Epoch [54/100], Batch [30/35], Loss: 1.7158\n",
            "Epoch [54/100] Average Loss: 2.1600\n",
            "\n",
            "Epoch [55/100], Batch [10/35], Loss: 2.4021\n",
            "Epoch [55/100], Batch [20/35], Loss: 2.3251\n",
            "Epoch [55/100], Batch [30/35], Loss: 1.6753\n",
            "Epoch [55/100] Average Loss: 2.1151\n",
            "\n",
            "Epoch [56/100], Batch [10/35], Loss: 1.9944\n",
            "Epoch [56/100], Batch [20/35], Loss: 1.9983\n",
            "Epoch [56/100], Batch [30/35], Loss: 2.2586\n",
            "Epoch [56/100] Average Loss: 2.0800\n",
            "\n",
            "Epoch [57/100], Batch [10/35], Loss: 2.3123\n",
            "Epoch [57/100], Batch [20/35], Loss: 1.8714\n",
            "Epoch [57/100], Batch [30/35], Loss: 2.2285\n",
            "Epoch [57/100] Average Loss: 2.0685\n",
            "\n",
            "Epoch [58/100], Batch [10/35], Loss: 1.9681\n",
            "Epoch [58/100], Batch [20/35], Loss: 2.1184\n",
            "Epoch [58/100], Batch [30/35], Loss: 2.5275\n",
            "Epoch [58/100] Average Loss: 2.0465\n",
            "\n",
            "Epoch [59/100], Batch [10/35], Loss: 1.7983\n",
            "Epoch [59/100], Batch [20/35], Loss: 2.1166\n",
            "Epoch [59/100], Batch [30/35], Loss: 1.9822\n",
            "Epoch [59/100] Average Loss: 2.0029\n",
            "\n",
            "Epoch [60/100], Batch [10/35], Loss: 1.8389\n",
            "Epoch [60/100], Batch [20/35], Loss: 1.8867\n",
            "Epoch [60/100], Batch [30/35], Loss: 2.2636\n",
            "Epoch [60/100] Average Loss: 1.9471\n",
            "\n",
            "Epoch [61/100], Batch [10/35], Loss: 2.1099\n",
            "Epoch [61/100], Batch [20/35], Loss: 1.7814\n",
            "Epoch [61/100], Batch [30/35], Loss: 1.8379\n",
            "Epoch [61/100] Average Loss: 1.9226\n",
            "\n",
            "Epoch [62/100], Batch [10/35], Loss: 1.7311\n",
            "Epoch [62/100], Batch [20/35], Loss: 1.8014\n",
            "Epoch [62/100], Batch [30/35], Loss: 1.8398\n",
            "Epoch [62/100] Average Loss: 1.8690\n",
            "\n",
            "Epoch [63/100], Batch [10/35], Loss: 1.9134\n",
            "Epoch [63/100], Batch [20/35], Loss: 1.9252\n",
            "Epoch [63/100], Batch [30/35], Loss: 1.5677\n",
            "Epoch [63/100] Average Loss: 1.7702\n",
            "\n",
            "Epoch [64/100], Batch [10/35], Loss: 1.3919\n",
            "Epoch [64/100], Batch [20/35], Loss: 1.8337\n",
            "Epoch [64/100], Batch [30/35], Loss: 2.1469\n",
            "Epoch [64/100] Average Loss: 1.6691\n",
            "\n",
            "Epoch [65/100], Batch [10/35], Loss: 1.5088\n",
            "Epoch [65/100], Batch [20/35], Loss: 1.4006\n",
            "Epoch [65/100], Batch [30/35], Loss: 1.5146\n",
            "Epoch [65/100] Average Loss: 1.5442\n",
            "\n",
            "Epoch [66/100], Batch [10/35], Loss: 1.6129\n",
            "Epoch [66/100], Batch [20/35], Loss: 1.6690\n",
            "Epoch [66/100], Batch [30/35], Loss: 1.7701\n",
            "Epoch [66/100] Average Loss: 1.4595\n",
            "\n",
            "Epoch [67/100], Batch [10/35], Loss: 1.5259\n",
            "Epoch [67/100], Batch [20/35], Loss: 0.8847\n",
            "Epoch [67/100], Batch [30/35], Loss: 1.2621\n",
            "Epoch [67/100] Average Loss: 1.3967\n",
            "\n",
            "Epoch [68/100], Batch [10/35], Loss: 1.2599\n",
            "Epoch [68/100], Batch [20/35], Loss: 1.2972\n",
            "Epoch [68/100], Batch [30/35], Loss: 1.3278\n",
            "Epoch [68/100] Average Loss: 1.3175\n",
            "\n",
            "Epoch [69/100], Batch [10/35], Loss: 1.3017\n",
            "Epoch [69/100], Batch [20/35], Loss: 1.2620\n",
            "Epoch [69/100], Batch [30/35], Loss: 1.1358\n",
            "Epoch [69/100] Average Loss: 1.1990\n",
            "\n",
            "Epoch [70/100], Batch [10/35], Loss: 1.1503\n",
            "Epoch [70/100], Batch [20/35], Loss: 1.1097\n",
            "Epoch [70/100], Batch [30/35], Loss: 1.1776\n",
            "Epoch [70/100] Average Loss: 1.0806\n",
            "\n",
            "Epoch [71/100], Batch [10/35], Loss: 0.5902\n",
            "Epoch [71/100], Batch [20/35], Loss: 1.0399\n",
            "Epoch [71/100], Batch [30/35], Loss: 1.3531\n",
            "Epoch [71/100] Average Loss: 1.0438\n",
            "\n",
            "Epoch [72/100], Batch [10/35], Loss: 0.7180\n",
            "Epoch [72/100], Batch [20/35], Loss: 0.9845\n",
            "Epoch [72/100], Batch [30/35], Loss: 1.0051\n",
            "Epoch [72/100] Average Loss: 0.9610\n",
            "\n",
            "Epoch [73/100], Batch [10/35], Loss: 1.0543\n",
            "Epoch [73/100], Batch [20/35], Loss: 0.9781\n",
            "Epoch [73/100], Batch [30/35], Loss: 1.1304\n",
            "Epoch [73/100] Average Loss: 0.9025\n",
            "\n",
            "Epoch [74/100], Batch [10/35], Loss: 0.6046\n",
            "Epoch [74/100], Batch [20/35], Loss: 0.9267\n",
            "Epoch [74/100], Batch [30/35], Loss: 0.9535\n",
            "Epoch [74/100] Average Loss: 0.8111\n",
            "\n",
            "Epoch [75/100], Batch [10/35], Loss: 0.7393\n",
            "Epoch [75/100], Batch [20/35], Loss: 0.9586\n",
            "Epoch [75/100], Batch [30/35], Loss: 0.7120\n",
            "Epoch [75/100] Average Loss: 0.7881\n",
            "\n",
            "Epoch [76/100], Batch [10/35], Loss: 0.6647\n",
            "Epoch [76/100], Batch [20/35], Loss: 0.7174\n",
            "Epoch [76/100], Batch [30/35], Loss: 1.0446\n",
            "Epoch [76/100] Average Loss: 0.8205\n",
            "\n",
            "Epoch [77/100], Batch [10/35], Loss: 0.7247\n",
            "Epoch [77/100], Batch [20/35], Loss: 0.7122\n",
            "Epoch [77/100], Batch [30/35], Loss: 0.7999\n",
            "Epoch [77/100] Average Loss: 0.6647\n",
            "\n",
            "Epoch [78/100], Batch [10/35], Loss: 0.6418\n",
            "Epoch [78/100], Batch [20/35], Loss: 0.7393\n",
            "Epoch [78/100], Batch [30/35], Loss: 0.4055\n",
            "Epoch [78/100] Average Loss: 0.6146\n",
            "\n",
            "Epoch [79/100], Batch [10/35], Loss: 0.6255\n",
            "Epoch [79/100], Batch [20/35], Loss: 0.6471\n",
            "Epoch [79/100], Batch [30/35], Loss: 0.5449\n",
            "Epoch [79/100] Average Loss: 0.5810\n",
            "\n",
            "Epoch [80/100], Batch [10/35], Loss: 0.6074\n",
            "Epoch [80/100], Batch [20/35], Loss: 0.3559\n",
            "Epoch [80/100], Batch [30/35], Loss: 0.5842\n",
            "Epoch [80/100] Average Loss: 0.5389\n",
            "\n",
            "Epoch [81/100], Batch [10/35], Loss: 0.4738\n",
            "Epoch [81/100], Batch [20/35], Loss: 0.6978\n",
            "Epoch [81/100], Batch [30/35], Loss: 0.6537\n",
            "Epoch [81/100] Average Loss: 0.5190\n",
            "\n",
            "Epoch [82/100], Batch [10/35], Loss: 0.4113\n",
            "Epoch [82/100], Batch [20/35], Loss: 0.6110\n",
            "Epoch [82/100], Batch [30/35], Loss: 0.6890\n",
            "Epoch [82/100] Average Loss: 0.5171\n",
            "\n",
            "Epoch [83/100], Batch [10/35], Loss: 0.3497\n",
            "Epoch [83/100], Batch [20/35], Loss: 0.5832\n",
            "Epoch [83/100], Batch [30/35], Loss: 0.4327\n",
            "Epoch [83/100] Average Loss: 0.5111\n",
            "\n",
            "Epoch [84/100], Batch [10/35], Loss: 0.2583\n",
            "Epoch [84/100], Batch [20/35], Loss: 0.5395\n",
            "Epoch [84/100], Batch [30/35], Loss: 0.3312\n",
            "Epoch [84/100] Average Loss: 0.4838\n",
            "\n",
            "Epoch [85/100], Batch [10/35], Loss: 0.5690\n",
            "Epoch [85/100], Batch [20/35], Loss: 0.6532\n",
            "Epoch [85/100], Batch [30/35], Loss: 0.0592\n",
            "Epoch [85/100] Average Loss: 0.4665\n",
            "\n",
            "Epoch [86/100], Batch [10/35], Loss: 0.3775\n",
            "Epoch [86/100], Batch [20/35], Loss: 0.4692\n",
            "Epoch [86/100], Batch [30/35], Loss: 0.4137\n",
            "Epoch [86/100] Average Loss: 0.4566\n",
            "\n",
            "Epoch [87/100], Batch [10/35], Loss: 0.2308\n",
            "Epoch [87/100], Batch [20/35], Loss: 0.3764\n",
            "Epoch [87/100], Batch [30/35], Loss: 0.4509\n",
            "Epoch [87/100] Average Loss: 0.4084\n",
            "\n",
            "Epoch [88/100], Batch [10/35], Loss: 0.4233\n",
            "Epoch [88/100], Batch [20/35], Loss: 0.4404\n",
            "Epoch [88/100], Batch [30/35], Loss: 0.2341\n",
            "Epoch [88/100] Average Loss: 0.4000\n",
            "\n",
            "Epoch [89/100], Batch [10/35], Loss: 0.4775\n",
            "Epoch [89/100], Batch [20/35], Loss: 0.4938\n",
            "Epoch [89/100], Batch [30/35], Loss: 0.2373\n",
            "Epoch [89/100] Average Loss: 0.4005\n",
            "\n",
            "Epoch [90/100], Batch [10/35], Loss: 0.4703\n",
            "Epoch [90/100], Batch [20/35], Loss: 0.3814\n",
            "Epoch [90/100], Batch [30/35], Loss: 0.3493\n",
            "Epoch [90/100] Average Loss: 0.3991\n",
            "\n",
            "Epoch [91/100], Batch [10/35], Loss: 0.3380\n",
            "Epoch [91/100], Batch [20/35], Loss: 0.3401\n",
            "Epoch [91/100], Batch [30/35], Loss: 0.5475\n",
            "Epoch [91/100] Average Loss: 0.3691\n",
            "\n",
            "Epoch [92/100], Batch [10/35], Loss: 0.3637\n",
            "Epoch [92/100], Batch [20/35], Loss: 0.2140\n",
            "Epoch [92/100], Batch [30/35], Loss: 0.3165\n",
            "Epoch [92/100] Average Loss: 0.3730\n",
            "\n",
            "Epoch [93/100], Batch [10/35], Loss: 0.3234\n",
            "Epoch [93/100], Batch [20/35], Loss: 0.4080\n",
            "Epoch [93/100], Batch [30/35], Loss: 1.3142\n",
            "Epoch [93/100] Average Loss: 0.5860\n",
            "\n",
            "Epoch [94/100], Batch [10/35], Loss: 0.7418\n",
            "Epoch [94/100], Batch [20/35], Loss: 0.5990\n",
            "Epoch [94/100], Batch [30/35], Loss: 0.6514\n",
            "Epoch [94/100] Average Loss: 0.6740\n",
            "\n",
            "Epoch [95/100], Batch [10/35], Loss: 0.5215\n",
            "Epoch [95/100], Batch [20/35], Loss: 0.5548\n",
            "Epoch [95/100], Batch [30/35], Loss: 0.5209\n",
            "Epoch [95/100] Average Loss: 0.5911\n",
            "\n",
            "Epoch [96/100], Batch [10/35], Loss: 0.2458\n",
            "Epoch [96/100], Batch [20/35], Loss: 0.2831\n",
            "Epoch [96/100], Batch [30/35], Loss: 0.5754\n",
            "Epoch [96/100] Average Loss: 0.4161\n",
            "\n",
            "Epoch [97/100], Batch [10/35], Loss: 0.2685\n",
            "Epoch [97/100], Batch [20/35], Loss: 0.3629\n",
            "Epoch [97/100], Batch [30/35], Loss: 0.5225\n",
            "Epoch [97/100] Average Loss: 0.3353\n",
            "\n",
            "Epoch [98/100], Batch [10/35], Loss: 0.2444\n",
            "Epoch [98/100], Batch [20/35], Loss: 0.2424\n",
            "Epoch [98/100], Batch [30/35], Loss: 0.3554\n",
            "Epoch [98/100] Average Loss: 0.2826\n",
            "\n",
            "Epoch [99/100], Batch [10/35], Loss: 0.2419\n",
            "Epoch [99/100], Batch [20/35], Loss: 0.2140\n",
            "Epoch [99/100], Batch [30/35], Loss: 0.3198\n",
            "Epoch [99/100] Average Loss: 0.2564\n",
            "\n",
            "Epoch [100/100], Batch [10/35], Loss: 0.2037\n",
            "Epoch [100/100], Batch [20/35], Loss: 0.1993\n",
            "Epoch [100/100], Batch [30/35], Loss: 0.2071\n",
            "Epoch [100/100] Average Loss: 0.2209\n",
            "\n",
            "Training complete and model saved as 'hybrid_ocr_transformer.pth'.\n",
            "\n",
            "Evaluating model on individual samples (CER & WER):\n",
            "Filename: Buendia transcription_page_1.jpg\n",
            "Ground Truth: NOTES:\t\tu and v are used interchangeably \tcheck against dictionary?\n",
            "\t\ttwo types of lowercase “s” -> ‘s’ and ‘ſ’  both should be transcribed as ‘s’\n",
            "\t\taccents are inconsistent \t\tshould be ignored (except ñ)\n",
            "\t\tsome letters have macrons (¯)\t\tshould mean ‘n’ follows, or ‘ue’ after capped q\n",
            "\t\tsome line end hyphens not present\tleave words split for now, can decide later\n",
            "\t\tç old spelling is always modern z\tteach AI to always interpret ç as z\n",
            "Predicted   : ntetdtetestni r cq oar d d si tsleureereas\n",
            "CER: 0.908, WER: 1.000\n",
            "\n",
            "Filename: Buendia transcription_page_10.jpg\n",
            "Ground Truth: CENSURA DEL R. P. ANTONIO CO-\n",
            "dorniu de la Compañía de jesus, Maes-\n",
            "tro que fue de Theologia, Examinador\n",
            "Synodal de los Obispados de Gerona, Ur-\n",
            "gel, y Barcelona, Oc.\n",
            "DE orden del Ilustre Señor Don Fran-\n",
            "cisco Drechos, Canonigo, y Sacristan\n",
            "Dignidad de la Santa Iglesia de Gerona, y\n",
            "Vicario General por el Ilustrissimo Señor\n",
            "D. Balthasar de Bastero y lledo, Obispo\n",
            "de Gerona, del Consejo de su Magestad, & c. \n",
            "He visto un Librito, cuyo titulo es: Ins-\n",
            "truccion de Christiana, y Politica Cortesa-\n",
            "nia, Oc. Su Author D. Fausto Agustin de\n",
            "Buendia, Colegial que fue en el Imperial\n",
            "de Cordellas, &c. Y brevemente digo, \n",
            "no solo que nada contiene contra la Fe, y \n",
            "buenas costumbres, sino que muy atento\n",
            "el Author con entrambas, describe, y en-\n",
            "seña tan culta, y discreta la Virtud, co-\n",
            "mo santa la Policia, y Urbanidad. Los\n",
            "Señoritos, que se criaren con estos do-\n",
            "cumentos, mereceran, quando hombres, \n",
            "aver nacido Señores. Porque no solo sa-\n",
            "bran ser Caballeros, sino tambien a lo\n",
            "Predicted   : toenuitepoi deynee n l doere r  ttii\n",
            "CER: 0.963, WER: 1.000\n",
            "\n",
            "Filename: Buendia transcription_page_2.jpg\n",
            "Ground Truth: PDF p1\n",
            "Predicted   : pdf p2\n",
            "CER: 0.667, WER: 1.000\n",
            "\n",
            "Filename: Buendia transcription_page_3.jpg\n",
            "Ground Truth: Al\n",
            "INFINITAMENTE AMABLE\n",
            "NIÑO JESUS.\n",
            "Predicted   : alininitamente amableniño jesus\n",
            "CER: 0.914, WER: 1.000\n",
            "\n",
            "Filename: Buendia transcription_page_4.jpg\n",
            "Ground Truth: A Vos, Dulcissimo Niño\n",
            "JESUS, que no solo os\n",
            "dignasteis de llamaros\n",
            "Doctor de los Niños, \n",
            "sino también de assis-\n",
            "tir como Niño entre los Doctores, \n",
            "se consagra humilde esta pequeña\n",
            "Instrucción de los Niños. Es assi, \n",
            "que ella también se dirige a la ju-\n",
            "ventud; pero a esta, como recuer-\n",
            "do de lo que aprendió, a los Ni-\n",
            "ños, como precisa explicacion de \n",
            "lo que deben estudiar. Por este so-\n",
            "lo titulo es muy vuestra; y por\n",
            "ser para Niños, que confiais a la\n",
            "educacion de vuestra Compañia, \n",
            "lo es mucho mas. En Vos, (Divi-\n",
            "no Exemplar de todas las virtu-\n",
            "des) tienen abreviado el mas se-\n",
            "Predicted   : sennttio ce er  daec nne teaaso\n",
            "CER: 0.947, WER: 1.000\n",
            "\n",
            "Filename: Buendia transcription_page_5.jpg\n",
            "Ground Truth: PDF p2\n",
            "Predicted   : pdf p2\n",
            "CER: 0.500, WER: 0.500\n",
            "\n",
            "Filename: Buendia transcription_page_6.jpg\n",
            "Ground Truth: guro disseño de su edad: la Reli-\n",
            "gion para con Dios en la devota\n",
            "assistencia a los Templos; la piedad\n",
            "con los Padres en la obediencia\n",
            "mas rendida; y la modestia, y de-\n",
            "seo de saber, con los mayores, \n",
            "gustando mas de oir, y pregun-\n",
            "tar, que de definir, y resolver. Bien\n",
            "que esto en vuestra infinita Sabi-\n",
            "duria fue soberana dignacion, y\n",
            "en la natural ignorancia de los\n",
            "Niños es indispensable necessi-\n",
            "dad. \n",
            "Ni tienen solamente en Vos \n",
            "el disseño, la luz, y el exemplo, \n",
            "sino tambien el amor, y protec-\n",
            "cion. Vos, como singular Maes-\n",
            "tro de los Niños, les dais enten-\n",
            "dimiento, y comunicais la sabi-\n",
            "duria. Vos les prometeis el Reyno\n",
            "de los Cielos, y os indignais con\n",
            "quien les aparta de Vos, y les\n",
            "proponeis por norma del can-\n",
            "dor, inocencia, y christiana hu-\n",
            "mildad. Vuestro amor parece que\n",
            "no pudo explicarse mas tierno, y\n",
            "liberal con los Niños, pues no\n",
            "contento de echarles vuestras di-\n",
            "Predicted   : aeen tn de da   ci\n",
            "CER: 0.980, WER: 0.994\n",
            "\n",
            "Filename: Buendia transcription_page_7.jpg\n",
            "Ground Truth: vinas bendiciones, les unisteis\n",
            "a vuestro sagrado pecho con sua-\n",
            "vissimos abrazos. Dichosa edad, \n",
            "que os merecio tan regalados cariños!\n",
            "Y pues en la celestial Jeru-\n",
            "salen no ha mudado de condicion\n",
            "vuestra Benignidad, proseguid, \n",
            "o Niño tierno, y Dios Eterno, \n",
            "proseguid en bendecirles, y favo-\n",
            "recerles. Sean tan fervorosamen-\n",
            "te devotos de vuestra Admirable\n",
            "MADRE, que se porten como sus\n",
            "hijos, y hermanos de leche con\n",
            "Vos. Seran sabios, si fueren cas-\n",
            "tos; que no entra vuestra Sabi-\n",
            "duria, donde no ay mucha pure-\n",
            "za de conciencia. Crezcan en\n",
            "vuestro santo temor, y amor, co-\n",
            "como en los años, y mucho mas. \n",
            "Adelantense en la virtud, como\n",
            "en las letras, y mucho mas; has-\n",
            "ta que lleguen, por vuesetra imi-\n",
            "tacion, a ser varones perfectos, \n",
            "y consumados, agradables a\n",
            "vuestros ojos, y provechosos a\n",
            "la Republica, que libra casi to-\n",
            "da su felizidad en la acertada\n",
            "Predicted   : p t2\n",
            "CER: 0.997, WER: 1.000\n",
            "\n",
            "Filename: Buendia transcription_page_8.jpg\n",
            "Ground Truth: PDF p3\n",
            "Predicted   : pdf p2\n",
            "CER: 0.667, WER: 1.000\n",
            "\n",
            "Filename: Buendia transcription_page_9.jpg\n",
            "Ground Truth: crianza de la niñez. Assi sea, \n",
            "Divinissimo Niño, por vuestra\n",
            "gracia, assi sea, a vuestra ma-\n",
            "yor gloria. Amen.\n",
            "Predicted   : aetle ili ptmv de geeo\n",
            "CER: 0.865, WER: 1.000\n",
            "\n",
            "Filename: Constituciones sinodales transcription_page_1.jpg\n",
            "Ground Truth: NOTES:\t\tu and v are used interchangeably \tcheck against dictionary?\n",
            "\t\ttwo types of lowercase “s” -> ‘s’ and ‘ſ’  both should be transcribed as ‘s’\n",
            "\t\taccents are inconsistent \t\tshould be ignored (except ñ)\n",
            "\t\tsome letters have macrons (¯)\t\tshould mean ‘n’ follows, or ‘ue’ after capped q\n",
            "\t\tsome line end hyphens not present\tleave words split for now, can decide later\n",
            "\t\tç old spelling is always modern z\tteach AI to always interpret ç as z\n",
            "Predicted   : ntetdtetestni r cq oar d d si tsleureereas\n",
            "CER: 0.908, WER: 1.000\n",
            "\n",
            "Filename: Constituciones sinodales transcription_page_10.jpg\n",
            "Ground Truth: PDF p3\n",
            "Predicted   : pdf p2\n",
            "CER: 0.667, WER: 1.000\n",
            "\n",
            "Filename: Constituciones sinodales transcription_page_11.jpg\n",
            "Ground Truth: DON Pedro Manso por la gracia de Dios\n",
            "y de la sancta Yglesia de Roma, Obispo\n",
            "de Calahorra, y la Calzada, del Consejo, \n",
            "de su Magestad, &c. Al dean y Cavil-\n",
            "dos de las nuestras sanctas yglesias Ca-\n",
            "thedrales de Calahorra, y la Calzada, y\n",
            "Cavildos de las yglesias Collegiales, Ar\n",
            "ciprestes, Vicarias, Universidades, ygle-\n",
            "sias unidas, Cavildos parrochiales, Curas, Beneficiados, y\n",
            "Clerigos, Mayordomos, Administradores de Cofradias, Her\n",
            "mandades de confrades, y vezindades, e a los Concejos, Seño\n",
            "rios, juntas de Provincias, justicias, quanto a lo espiritual, y\n",
            "otras qualesquier personas Ecclesiasticas, o seglares: y a to-\n",
            "das las yglesias, hospitales, confradias y lugares pios, de todas\n",
            "las ciudades, villas, y lugares deste nuestro Obispado; y a to-\n",
            "dos los demas que de derecho y costumbre, o en otra qual-\n",
            "quier manera soys obligados y os conviene venir al Synodo\n",
            "Diocessano: y q os fuere notificado este nuestro mandamien\n",
            "to en vuestras personas, o en vuestrar yglesias, o como del par\n",
            "te supieredes, de manera que no podays pretender ignoran-\n",
            "cia. Salud y bendicion en nuestro Señor Jesu Christo: sabed, \n",
            "que por estar mandado por el sancto Concilio de Trento, que\n",
            "los Prelados en cada un año hagan Synodo Diocessano, pa-\n",
            "ra estatuyr lo qe se dispone en sus decretos, y sacros Cano-\n",
            "nes, hazer justicia, deshazer agravios, reformar costumbres, ha-\n",
            "zer constituciones, para que el culto Divino vaya en augmen\n",
            "to, y las haziendas de las Fabricas y obras pias se conserven, con\n",
            "el favor Divino havemos acordado celebrar Synodo en la \n",
            "yglesia Collegial de sancta Maria la Redonda de la ciudad de\n",
            "Logroño, y se comenzara a doze dias del mes de Noviembre\n",
            "de este presente año de seyscientos, en razon de lo dicho y descar\n",
            "go de nuestra conciencia. Por ende por las presentes y su te-\n",
            "nor os citamos, notificamos y llamamos, y (si necessario es)\n",
            "mandamos en virtud de sancta obediencia, y so pena de exco\n",
            "munion, trina, canonica monitione premissa, y cada cien du-\n",
            "cados, aplicados para obras pias a nuestra disposicion: que ven-\n",
            "gays a os hallar y estar presentes al dicho Synodo (y los Cle-\n",
            "rigos con habitos decentes, en bestidos, cabellos, y barba, y\n",
            "Predicted   : tei   tn\n",
            "CER: 0.996, WER: 1.000\n",
            "\n",
            "Filename: Constituciones sinodales transcription_page_2.jpg\n",
            "Ground Truth: PDF p1\n",
            "Predicted   : pdf p2\n",
            "CER: 0.667, WER: 1.000\n",
            "\n",
            "Filename: Constituciones sinodales transcription_page_3.jpg\n",
            "Ground Truth: DON PHELIPPE POR LA\n",
            "Gracia de Dios, Rey de Castilla, de\n",
            "Leon, de Aragon, de las dos Sici-\n",
            "lias, de Hierusalem, de Portugal, de\n",
            "Navarra, de Granada, de Toledo, \n",
            "de Valencia, de Galizia, de Mallorca, \n",
            "de Sevilla, de Cerdeña, de Cordova, \n",
            "de Corcega, de Murcia, de Jaen, de\n",
            "los Algarves, de Algecira, de Gibraltar, de las Islas de\n",
            "Canaria, de las Indias Orientales, y Occidentales, Islas\n",
            "y tierra firme del mar Oceano, Archiduque de Austria, \n",
            "Duque de Borgoña, de Bravante, y Milan, Conde de Abs\n",
            "purg, de Flandes, y de Tirol, señor de Vizcaya, y de Mo-\n",
            "lina, &c. Por quanto por parte de vos, el Reverendo in\n",
            "Christo Padre, don Pedro Manso, Obispo de Calahorra, \n",
            "y la Calzada, del nuestro Consejo: nos fue hecha relacion\n",
            "que en un Synodo que se havia hecho en la ciudad de Lo-\n",
            "grono, de esse Obispado, se havian hecho algunas Consti-\n",
            "tuciones Synodales, y reformadas las antiguas, y nos fue\n",
            "pedido, y suplicado os mandasemos dar licencia para que\n",
            "se imprimiessen las dichas constituciones, y lo pudiesse\n",
            "hazer qualquier impressor de estos nuestros Reynos que\n",
            "vos nombrasedes, o como la nuestra merced fuesse. Lo\n",
            "qual visto por los del nuestro Consejo, y lo pedido cerca \n",
            "de ello por el Licenciado Gil Ramirez de Arellano nue-\n",
            "stro Fiscal, y la contradizion fecha, por parte de la Pro-\n",
            "vincia y. hermandades de Alava, fue acordado que devia-\n",
            "mos mandar dar esta nuestra carta en la dicha razon, y\n",
            "nos tuvimos lo por bien. Por la qual vos damos licencia\n",
            "y facultad para que qualquier impressor de estos nuestros\n",
            "Reynos, que vos nombraredes, pueda imprimir las di-\n",
            "chase Constituciones Synodales, con que despues de im-\n",
            "pressas no se pueda usar de ellas antes que se traygan an-\n",
            "te nos. y se corrijan con el original que va rubricado y fir-\n",
            "mado al fin de ellas de Christoval Nuñez de Leon, nuestro\n",
            "escrivano de Camara. de los que residen en el nuestro Con\n",
            "sejo, y se tasse el precio a que se huviere de vender cada\n",
            "pliego de ellas, so pena de caer e incurrir en las penas, con\n",
            "Predicted   : t ii   to\n",
            "CER: 0.995, WER: 1.000\n",
            "\n",
            "Filename: Constituciones sinodales transcription_page_4.jpg\n",
            "Ground Truth: PDF p2\n",
            "Predicted   : pdf p2\n",
            "CER: 0.500, WER: 0.500\n",
            "\n",
            "Filename: Constituciones sinodales transcription_page_5.jpg\n",
            "Ground Truth: tenidas en la pragmatica y leyes de nuestros Reynos, que\n",
            "disponen sobre la impression de los libros, y nos agades en\n",
            "de al. De lo qual mandamos dar, y dimos esta nuestra car-\n",
            "ta, sellada con nuestro sello, y librada por los de nuestro\n",
            "Consejo. Dada en la ciudad de Valladolid, a siete dias del\n",
            "mes de Septiembre, de mil y seyscientos y un años.\n",
            "Predicted   : setsntetesva r esrs ttcisilaearemeo\n",
            "CER: 0.904, WER: 1.000\n",
            "\n",
            "Filename: Constituciones sinodales transcription_page_6.jpg\n",
            "Ground Truth: El Conde de\tEl Licenciado don\tEl Licenciado don Die-\n",
            "Miranda.\tIvan de Acuña. \t\tgo Lopez de Ayala.\n",
            "Predicted   : coteto t o ipedagdr ley  puouesara a iemeln\n",
            "CER: 0.804, WER: 1.000\n",
            "\n",
            "Filename: Constituciones sinodales transcription_page_7.jpg\n",
            "Ground Truth: El Licenciado don\tel Licenciado don Francisco\n",
            "Ivan de Ocon. \t\tde Contreras.\n",
            "Predicted   : pi eolioio ledsdrn t peeonl ninn\n",
            "CER: 0.773, WER: 1.000\n",
            "\n",
            "Filename: Constituciones sinodales transcription_page_8.jpg\n",
            "Ground Truth: Yo Christoval Nuñez de Leon, escrivano de Camara\n",
            "del Rey nuestro señor, la fize escribir por su mandado, con\n",
            "acuerdo de los del su Consejo.\n",
            "Predicted   : noteulinceyr et sr r aptp asye o nemcueriin\n",
            "CER: 0.806, WER: 1.000\n",
            "\n",
            "Filename: Constituciones sinodales transcription_page_9.jpg\n",
            "Ground Truth: Registrada Jorge de\tChanciller Jorge de\n",
            "Olaalde Vergara. \tOlaalde Vergara.\n",
            "Predicted   : aetneinnor dednadesaonoe di eto\n",
            "CER: 0.784, WER: 1.000\n",
            "\n",
            "Filename: Ezcaray transcription_page_1.jpg\n",
            "Ground Truth: NOTES:\t\tu and v are used interchangeably \tcheck against dictionary?\n",
            "\t\ttwo types of lowercase “s” -> ‘s’ and ‘ſ’  both should be transcribed as ‘s’\n",
            "\t\taccents are inconsistent \t\tshould be ignored (except ñ)\n",
            "\t\tsome letters have macrons (¯)\t\tshould mean ‘n’ follows, or ‘ue’ after capped q\n",
            "\t\tsome line end hyphens not present\tleave words split for now, can decide later\n",
            "\t\tç old spelling is always modern z\tteach AI to always interpret ç as z\n",
            "Predicted   : ntetdtetestni r cq oar d d si tsleureereas\n",
            "CER: 0.908, WER: 1.000\n",
            "\n",
            "Filename: Ezcaray transcription_page_2.jpg\n",
            "Ground Truth: PDF p1\n",
            "Predicted   : pdf p2\n",
            "CER: 0.667, WER: 1.000\n",
            "\n",
            "Filename: Ezcaray transcription_page_3.jpg\n",
            "Ground Truth: SEÑOR ILUSTRISSIMO\n",
            "Predicted   : seññor ilustrissimo\n",
            "CER: 1.000, WER: 1.000\n",
            "\n",
            "Filename: Ezcaray transcription_page_4.jpg\n",
            "Ground Truth: Ocupado en el exercicio\n",
            "de las Missiones en el \n",
            "Obispado de Guadala-\n",
            "xara, recibi una de V.S.I.\n",
            "en que me da noticia de\n",
            "como su Magestad (que Dios guarde)\n",
            "se avia servido de honrarme con la\n",
            "merced de su Predicador; y como no\n",
            "se opone la predicacion de su Mages-\n",
            "tad a la Apostolica, tuve por de mi obli\n",
            "gacion admitir el favor, rindiendo a\n",
            "V.S.I. el agradecimiento. \n",
            "El Rey mi señor (que Dios guarde) \n",
            "hizo la gracia; mas a V.S.I. se le debe:\n",
            "que por mas frutos, que diera la tierra\n",
            "de Promission, no los lograra Moyses, \n",
            "si Josue, y Caleb no los sacassen. Dos\n",
            "sacaron el fruto, y de ambos necessito, \n",
            "para hallar un simil proporcionado a la \n",
            "grandeza de V.S.I.\n",
            "Predicted   : tenlnieoie duesrcervede deaed e bam conielo\n",
            "CER: 0.938, WER: 1.000\n",
            "\n",
            "Filename: Ezcaray transcription_page_5.jpg\n",
            "Ground Truth: PDF p2\n",
            "Predicted   : pdf p2\n",
            "CER: 0.500, WER: 0.500\n",
            "\n",
            "Filename: Ezcaray transcription_page_6.jpg\n",
            "Ground Truth: A estos dos nombres dan misterio-\n",
            "sas interpretaciones los Sagrados In-\n",
            "terpretes. A Caleb le llaman Quasi cor, \n",
            "y a Josue Dominus Salvator; Corazon, Señor, \n",
            "y Salvador. Y todos tres significados se\n",
            "hallan en V.I. siendo en el ministerio\n",
            "de patriarca el Aaron de Palacio, el Sa-\n",
            "cerdote grande de la Casa Real, en cu-\n",
            "yo pecho m ejor, que en el racional, se\n",
            "lee Verdad y Doctrina, para que como en\n",
            "animado Pectoral de discreciones, des-\n",
            "canse el corazon de su Magestad.\n",
            "Es tambien V.I. Corazon, Señor, y Salva-\n",
            "dor de toda la Christandad en los Rey-\n",
            "nos de España, peus por su ministerio, \n",
            "a imitacion del corazon, da vida espiri-\n",
            "tual a las almas para que se salven, re-\n",
            "partiendo la Bula de la Santa Cruzada \n",
            "a los fieles. \n",
            "Los primeros Comissarios de Cru-\n",
            "zada, que huvo, fueron Josue, y Caleb\n",
            "(no es arrojo de Predicador, sino inte-\n",
            "ligencia de Escripturario) pues en un\n",
            "Predicted   : pd  t2\n",
            "CER: 0.994, WER: 1.000\n",
            "\n",
            "Filename: Ezcaray transcription_page_7.jpg\n",
            "Ground Truth: PDF p3\n",
            "Predicted   : pdf p2\n",
            "CER: 0.667, WER: 1.000\n",
            "\n",
            "Filename: Ezcaray transcription_page_8.jpg\n",
            "Ground Truth: leño  a el ombro sacaron el razimo de\n",
            "la tierra de Promission, sombra, y figura\n",
            "de Jesu Christo nuestro Redemptor, \n",
            "pendiente del Sacrosanto Arbol, en\n",
            "cuya virtud se nos perdonan las cul-\n",
            "pas: esso es Bula, y porque todo nace\n",
            "de aquella preciosissima Sangre derra-\n",
            "mada en la Cruz, se llama Bula de la\n",
            "Santa Cruzada. Josue, y Caleb la sa-\n",
            "caron, y la publicaron a el pueblo; y a \n",
            "no ser incredulos huvieran entrado to-\n",
            "dos en la tierra de Promission. \n",
            "No acaso llamó el Sacro Texto\n",
            "Cerrojo (quem portaverunt in vecte) a el leño\n",
            "en quien pendia el razimo; porque si\n",
            "quitados los cerrojos se abren las puer\n",
            "tas, por aquel razimo pendiente se fran-\n",
            "queaba la entrada a la tierra de Promis-\n",
            "sion. Que es distribuir la bula, sino fa-\n",
            "cilitar la entrada a la Bienaventuranza?\n",
            "En esta desseo ver a V.S.I. y todo lo \n",
            "que no es esto, es nada. Y si a Josue, y \n",
            "Caleb su fe los introduxo en la tierra,\n",
            "Predicted   : ve nnone  dalet y de dos ar e teeuun\n",
            "CER: 0.960, WER: 0.982\n",
            "\n",
            "Filename: Mendo transcription_page_1.jpg\n",
            "Ground Truth: NOTES:\t\tu and v are used interchangeably \tcheck against dictionary?\n",
            "\t\ttwo types of lowercase “s” -> ‘s’ and ‘ſ’  both should be transcribed as ‘s’\n",
            "\t\taccents are inconsistent \t\tshould be ignored (except ñ)\n",
            "\t\tsome letter shave macrons (¯)\t\tshould mean ‘n’ follows, or ‘ue’ after capped q\n",
            "\t\tsome line end hyphens not present\tleave words split for now, can decide later\n",
            "\t\tç old spelling is always modern z\tteach AI to always interpret ç as z\n",
            "Predicted   : ntetdtetestni e c oar d d si tslerureereao\n",
            "CER: 0.906, WER: 1.000\n",
            "\n",
            "Filename: Mendo transcription_page_10.jpg\n",
            "Ground Truth: Eclesiastico Ordinario de su Real Capilla, Casa,\n",
            "Predicted   : eclesiastico ordinario de su real capilla casa\n",
            "CER: 0.146, WER: 0.714\n",
            "\n",
            "Filename: Mendo transcription_page_100.jpg\n",
            "Ground Truth: fas est, nam & in vestibulo suo inquirentem repellit objecta\n",
            "Predicted   : fas est nam in vestibulo suo inquirentem repellit objecta\n",
            "CER: 0.050, WER: 0.200\n",
            "\n",
            "Filename: Mendo transcription_page_101.jpg\n",
            "Ground Truth: verneratio; & si qui mentem propius adegerunt, quod oculis\n",
            "Predicted   : vrnero a si qui mentem propius adegerunt quod oculis\n",
            "CER: 0.121, WER: 0.333\n",
            "\n",
            "Filename: Mendo transcription_page_102.jpg\n",
            "Ground Truth: in Solem se contendentibus euenit, prestricta acie, videndi\n",
            "Predicted   : in so em se ontendentibus euenit prestricta acie videndi\n",
            "CER: 0.085, WER: 0.625\n",
            "\n",
            "Filename: Mendo transcription_page_103.jpg\n",
            "Ground Truth: facultate caruerunt. Verum si perfectus sit intuentem non\n",
            "Predicted   : facultute caruerunt verum si perfectus sit intuentem non\n",
            "CER: 0.053, WER: 0.375\n",
            "\n",
            "Filename: Mendo transcription_page_104.jpg\n",
            "Ground Truth: iniquus fulgor recundit, sed serenum lumen inuitat. No\n",
            "Predicted   : iniqus fulgor recundit sed serenum lumen inuitat no\n",
            "CER: 0.074, WER: 0.500\n",
            "\n",
            "Filename: Mendo transcription_page_105.jpg\n",
            "Ground Truth: desmayó este peligro el animo del Autor, para dejar de\n",
            "Predicted   : desmayó teste peligro el animo del autor para dejar de\n",
            "CER: 0.056, WER: 0.200\n",
            "\n",
            "Filename: Mendo transcription_page_106.jpg\n",
            "Ground Truth: proseguir esta Obra, en que yo hallo en grandecida la\n",
            "Predicted   : prsegrir rsta obra en que yo hallo en grandecida la\n",
            "CER: 0.094, WER: 0.300\n",
            "\n",
            "Filename: Mendo transcription_page_107.jpg\n",
            "Ground Truth: Religion; esmerada la Politica; la Cortesania ilustrada;\n",
            "Predicted   : relion esmerada la politica la cortesania ilustrada\n",
            "CER: 0.143, WER: 0.571\n",
            "\n",
            "Filename: Mendo transcription_page_108.jpg\n",
            "Ground Truth: las costumbres corregidas; la erudicion asi Sagrada, co-\n",
            "Predicted   : lascostbmbres corregidas la erudicion asi sagrada co\n",
            "CER: 0.107, WER: 0.625\n",
            "\n",
            "Filename: Mendo transcription_page_109.jpg\n",
            "Ground Truth: mo Historica, y Poëtica, primorosa; y tan selectamen-\n",
            "Predicted   : mo histirt y potica primorosa y tan selectamen\n",
            "CER: 0.208, WER: 0.500\n",
            "\n",
            "Filename: Mendo transcription_page_11.jpg\n",
            "Ground Truth: y Corte.\n",
            "Predicted   : y corte\n",
            "CER: 0.250, WER: 0.500\n",
            "\n",
            "Filename: Mendo transcription_page_110.jpg\n",
            "Ground Truth: te todo, que puedo dezir de su literatura, lo que del\n",
            "Predicted   : te tudo t que puedo dezir de su literatura lo que del\n",
            "CER: 0.075, WER: 0.273\n",
            "\n",
            "Filename: Mendo transcription_page_111.jpg\n",
            "Ground Truth: grande Basilio dijo Nazianzeno; Quod disciplina genus\n",
            "Predicted   : grande basili dijo nazianzeno quod disciplina genus\n",
            "CER: 0.094, WER: 0.429\n",
            "\n",
            "Filename: Mendo transcription_page_112.jpg\n",
            "Ground Truth: non calluit? imo quod non ea excellentia, tanquam ipsi vni-\n",
            "Predicted   : ntn c tlu timo quod non ea excellentia tanquam ipsi vni\n",
            "CER: 0.136, WER: 0.600\n",
            "\n",
            "Filename: Mendo transcription_page_113.jpg\n",
            "Ground Truth: animum adiecerit? Sic quidem omnes comprehendit, vt nul-\n",
            "Predicted   : a nimum aiecerit sic quidem omnes comprehendit vt nul\n",
            "CER: 0.107, WER: 0.750\n",
            "\n",
            "Filename: Mendo transcription_page_114.jpg\n",
            "Ground Truth: lus vnam; Sic ad Summum Singulas, vt mihi omuino\n",
            "Predicted   : lus vnam sin ad summum singulas vt mihi omuino\n",
            "CER: 0.125, WER: 0.444\n",
            "\n",
            "Filename: Mendo transcription_page_116.jpg\n",
            "Ground Truth: PDF p5\n",
            "Predicted   : pdf p2\n",
            "CER: 0.667, WER: 1.000\n",
            "\n",
            "Filename: Mendo transcription_page_117.jpg\n",
            "Ground Truth: videatur no curasse. No hay en todo el libro cosa contra\n",
            "Predicted   : videutur no eurasse no hay en todo el libro cosa contra\n",
            "CER: 0.071, WER: 0.273\n",
            "\n",
            "Filename: Mendo transcription_page_118.jpg\n",
            "Ground Truth: nuestra Santa Fe Catholica, ni que desdiga de lo moral\n",
            "Predicted   : nuestra santa fe catholica ni que desdiga de lo moral\n",
            "CER: 0.074, WER: 0.300\n",
            "\n",
            "Filename: Mendo transcription_page_119.jpg\n",
            "Ground Truth: en las costumbres; conque debe darsele la licencia, que\n",
            "Predicted   : en lts costumbres conque debe darsele la licencia que\n",
            "CER: 0.055, WER: 0.333\n",
            "\n",
            "Filename: Mendo transcription_page_120.jpg\n",
            "Ground Truth: pide. En este Collegio del Arzobispo de Toledo mi\n",
            "Predicted   : pide en este collegio del arzobispo de toledo mi\n",
            "CER: 0.102, WER: 0.556\n",
            "\n",
            "Filename: Mendo transcription_page_121.jpg\n",
            "Ground Truth: Señor, el Mayor de Salamanca, Iulio 3. de 1656.\n",
            "Predicted   : señor el mayor de salamanca iulio 3 de 1656\n",
            "CER: 0.170, WER: 0.667\n",
            "\n",
            "Filename: Mendo transcription_page_122.jpg\n",
            "Ground Truth: Doctor Don Francisco de Puga, y Feijoo.\n",
            "Predicted   : doctordon francisco de puga y feijoo\n",
            "CER: 0.205, WER: 0.714\n",
            "\n",
            "Filename: Mendo transcription_page_124.jpg\n",
            "Ground Truth: LICENCIA DEL ORDINARIO.\n",
            "Predicted   : licencia del ordinario\n",
            "CER: 0.913, WER: 1.000\n",
            "\n",
            "Filename: Mendo transcription_page_125.jpg\n",
            "Ground Truth: Don Iuan Perez Delgado por la gracia de Dios, y de\n",
            "Predicted   : don iuan perez delgado por la gracia de dios y de\n",
            "CER: 0.120, WER: 0.455\n",
            "\n",
            "Filename: Mendo transcription_page_126.jpg\n",
            "Ground Truth: la santa Sede Apostolica Obispo de Salamanca, del\n",
            "Predicted   : la santa sede apostolica obispo de salamanca del\n",
            "CER: 0.102, WER: 0.500\n",
            "\n",
            "Filename: Mendo transcription_page_127.jpg\n",
            "Ground Truth: Consejo de su Magestad, &c. Por la presente, por lo que\n",
            "Predicted   : consejo de su magestad c por la presente por lo que\n",
            "CER: 0.127, WER: 0.455\n",
            "\n",
            "Filename: Mendo transcription_page_128.jpg\n",
            "Ground Truth: a Nos toca, damos licencia, para que se pueda imprimir\n",
            "Predicted   : a not a oamos licencia para que se pueda imprimir\n",
            "CER: 0.148, WER: 0.400\n",
            "\n",
            "Filename: Mendo transcription_page_129.jpg\n",
            "Ground Truth: el Libro intitulado Principe Perfecto, y Ministros adjustados,\n",
            "Predicted   : el libro intitulado principe perfecto y ministros adjustados\n",
            "CER: 0.097, WER: 0.625\n",
            "\n",
            "Filename: Mendo transcription_page_13.jpg\n",
            "Ground Truth: SEGUNDA vez, (Illustrissimo Señor) Salen de\n",
            "Predicted   : seunda vez illustrissimo señor salen de\n",
            "CER: 0.302, WER: 0.833\n",
            "\n",
            "Filename: Mendo transcription_page_130.jpg\n",
            "Ground Truth: Documentos Politicos, y morales, conpuesto por el P. Andres\n",
            "Predicted   : doceueueos politicos y morales conpuesto por el p andres\n",
            "CER: 0.186, WER: 0.556\n",
            "\n",
            "Filename: Mendo transcription_page_131.jpg\n",
            "Ground Truth: Mendo de la Compañia de Iesus, Rector del Collegio\n",
            "Predicted   : mendo de la compañia de iesus rector del collegio\n",
            "CER: 0.120, WER: 0.556\n",
            "\n",
            "Filename: Mendo transcription_page_132.jpg\n",
            "Ground Truth: de Irlandeses de la Vniversidad desta dicha Ciudad; aten-\n",
            "Predicted   : de ir anreses de la vniversidad desta dicha ciudad aten\n",
            "CER: 0.123, WER: 0.556\n",
            "\n",
            "Filename: Mendo transcription_page_133.jpg\n",
            "Ground Truth: to, de la censura del Doctor Don Francisco de Puga, y\n",
            "Predicted   : to de la censura del doctor don francisco de puga y\n",
            "CER: 0.113, WER: 0.455\n",
            "\n",
            "Filename: Mendo transcription_page_134.jpg\n",
            "Ground Truth: Feijo, Colegial del Mayor del Arzobispo de Toledo de-\n",
            "Predicted   : feijo olegial del mayor del arzobispo de toledo de\n",
            "CER: 0.132, WER: 0.667\n",
            "\n",
            "Filename: Mendo transcription_page_135.jpg\n",
            "Ground Truth: sta dicha Vniversidad, y Cathedratico de Prima de Ca-\n",
            "Predicted   : sta dicha vniversidad y cathedratico de prima de ca\n",
            "CER: 0.113, WER: 0.444\n",
            "\n",
            "Filename: Mendo transcription_page_136.jpg\n",
            "Ground Truth: nones della, consta, no tiene cosa alguna contra nuestra\n",
            "Predicted   : nne lela consta no tiene cosa alguna contra nuestra\n",
            "CER: 0.107, WER: 0.333\n",
            "\n",
            "Filename: Mendo transcription_page_137.jpg\n",
            "Ground Truth: santa Fe Catholica, y buenas costumbres. Dada en Sala-\n",
            "Predicted   : santa afe catholica y buenas costumbres dada en sala\n",
            "CER: 0.148, WER: 0.556\n",
            "\n",
            "Filename: Mendo transcription_page_138.jpg\n",
            "Ground Truth: manca a seis dias del mes de Iulio de 1656. años.\n",
            "Predicted   : manca a seis dias del mes de iulio de 166 años\n",
            "CER: 0.082, WER: 0.273\n",
            "\n",
            "Filename: Mendo transcription_page_139.jpg\n",
            "Ground Truth: Iuan Obispo de Salamanca. Por mandato del Obispo de mi Señor.\n",
            "Predicted   : iuan ob spo de salamanca por mandato del obispo de mi señor\n",
            "CER: 0.148, WER: 0.636\n",
            "\n",
            "Filename: Mendo transcription_page_14.jpg\n",
            "Ground Truth: la estampa estos Documentos Politicos, y Morales\n",
            "Predicted   : la stampa estos documentos politicos y morales\n",
            "CER: 0.104, WER: 0.571\n",
            "\n",
            "Filename: Mendo transcription_page_140.jpg\n",
            "Ground Truth: Bartolome Fernandez Montojo.\n",
            "Predicted   : bartolome fernandez montojo\n",
            "CER: 0.143, WER: 1.000\n",
            "\n",
            "Filename: Mendo transcription_page_142.jpg\n",
            "Ground Truth: Licencia del R.P. Miguel de Arbizu Prouincial de la Com-\n",
            "Predicted   : licenciaidel rp miguel de arbizu prouincial de la com\n",
            "CER: 0.196, WER: 0.700\n",
            "\n",
            "Filename: Mendo transcription_page_143.jpg\n",
            "Ground Truth: pañia de Iesus, en la Prouincia de Castilla.\n",
            "Predicted   : pañia de iesus en la prouincia de castilla\n",
            "CER: 0.114, WER: 0.375\n",
            "\n",
            "Filename: Mendo transcription_page_144.jpg\n",
            "Ground Truth: Miguel de Arbizu Prouincial de la Compañia de Iesus\n",
            "Predicted   : miguel de arbizu prouincial de la compadia de iesus\n",
            "CER: 0.118, WER: 0.556\n",
            "\n",
            "Filename: Mendo transcription_page_145.jpg\n",
            "Ground Truth: en la Prouincia de Castilla, por especial comision, que\n",
            "Predicted   : en la prouinoua de castilla por especial comision que\n",
            "CER: 0.109, WER: 0.333\n",
            "\n",
            "Filename: Mendo transcription_page_146.jpg\n",
            "Ground Truth: para ello tengo de nuestro Padre Gosvvino NiKel Prepo-\n",
            "Predicted   : para aello tengo de nuestro padre gosvvino nikel prepo\n",
            "CER: 0.130, WER: 0.556\n",
            "\n",
            "Filename: Mendo transcription_page_147.jpg\n",
            "Ground Truth: sito Gerneral, por las presentes doy licencia al P. Andres\n",
            "Predicted   : sitr gern eral por las presentes doy licencia al p andres\n",
            "CER: 0.121, WER: 0.500\n",
            "\n",
            "Filename: Mendo transcription_page_148.jpg\n",
            "Ground Truth: Mendo Religioso de dicha Compañia, y Calificador de la\n",
            "Predicted   : mendo religioso de dicha compañia y calificador de la\n",
            "CER: 0.093, WER: 0.444\n",
            "\n",
            "Filename: Mendo transcription_page_149.jpg\n",
            "Ground Truth: Inquisicion Suprema, para que pueda imprimir vn Libro,\n",
            "Predicted   : inqioin suprema para que pueda imprimir vn libro\n",
            "CER: 0.185, WER: 0.375\n",
            "\n",
            "Filename: Mendo transcription_page_15.jpg\n",
            "Ground Truth: para formar vn Principe perfecto, y Ministros aju-\n",
            "Predicted   : para rormar vn principe perfecto y ministros aju\n",
            "CER: 0.100, WER: 0.625\n",
            "\n",
            "Filename: Mendo transcription_page_150.jpg\n",
            "Ground Truth: que ha compuesto, intitulado Principe Perfecto, y Ministros\n",
            "Predicted   : queha compuesto intitulado principe perfecto y ministros\n",
            "CER: 0.102, WER: 0.750\n",
            "\n",
            "Filename: Mendo transcription_page_151.jpg\n",
            "Ground Truth: ajustados, Documentos Politicos, y Morales, por auerle visto, y\n",
            "Predicted   : ajustados documentos politicos y morales por auerle visto y\n",
            "CER: 0.111, WER: 0.556\n",
            "\n",
            "Filename: Mendo transcription_page_152.jpg\n",
            "Ground Truth: aprobado personas graues, y doctas de nuestra Religion, a\n",
            "Predicted   : aprbar apersonas graues y doctas de nuestra religion a\n",
            "CER: 0.123, WER: 0.444\n",
            "\n",
            "Filename: Mendo transcription_page_153.jpg\n",
            "Ground Truth: quien le cometimos. En fe de lo qual, y para que dello con-\n",
            "Predicted   : quiei le oumeimos en fe de lo qual y para que dello con\n",
            "CER: 0.136, WER: 0.385\n",
            "\n",
            "Filename: Mendo transcription_page_154.jpg\n",
            "Ground Truth: ste, damos estas nuestras letras firmadas de nuestro nom-\n",
            "Predicted   : ste damos destas nuestras letras firmadasede nuestro nom\n",
            "CER: 0.070, WER: 0.556\n",
            "\n",
            "Filename: Mendo transcription_page_155.jpg\n",
            "Ground Truth: bre, y selladas con el sello de nuestro oficio. En Burgos a\n",
            "Predicted   : bre r selladas con el sello de nuestro oficio en burgos a\n",
            "CER: 0.085, WER: 0.417\n",
            "\n",
            "Filename: Mendo transcription_page_156.jpg\n",
            "Ground Truth: 27. de Nouiembre de 1656.      Miguel de Arbizu.\n",
            "Predicted   : 27 de nouiembre de 1656      miguel de arbizu\n",
            "CER: 0.125, WER: 0.625\n",
            "\n",
            "Filename: Mendo transcription_page_16.jpg\n",
            "Ground Truth: stados, por averse despachado en tiempo breve la Im-\n",
            "Predicted   : strados prra averse despachado en tiempo dreve la im\n",
            "CER: 0.135, WER: 0.444\n",
            "\n",
            "Filename: Mendo transcription_page_17.jpg\n",
            "Ground Truth: presion primera. Helos añadido de nuevo, y exornado\n",
            "Predicted   : pion piera helos añadido de nuevo y exornado\n",
            "CER: 0.157, WER: 0.500\n",
            "\n",
            "Filename: Mendo transcription_page_18.jpg\n",
            "Ground Truth: con estampas de Emblemas, que con mas halago de los ojos pongan a\n",
            "Predicted   : cos estpas t emblemas que con mas halago de los ojos pongan a\n",
            "CER: 0.108, WER: 0.308\n",
            "\n",
            "Filename: Mendo transcription_page_19.jpg\n",
            "Ground Truth: la vista las enseñanzas. Consagré a la Magestad Catolica de nuestro\n",
            "Predicted   : te sesis sie dhss sanepem rd rst odletinzcumad rnedli\n",
            "CER: 0.746, WER: 1.000\n",
            "\n",
            "Filename: Mendo transcription_page_2.jpg\n",
            "Ground Truth: PDF p1\n",
            "Predicted   : pdf p2\n",
            "CER: 0.667, WER: 1.000\n",
            "\n",
            "Filename: Mendo transcription_page_20.jpg\n",
            "Ground Truth: Monarca la primera vez este libro, y para que buelva mejorado a sus\n",
            "Predicted   : loaea a e qa yp mr a neaelr aqelq etoe irtci roeasros\n",
            "CER: 0.716, WER: 1.000\n",
            "\n",
            "Filename: Mendo transcription_page_21.jpg\n",
            "Ground Truth: Reales manos, le pongo en las V.S.I. de quien le admitira con los\n",
            "Predicted   : reles man s se pongo en las vsi de quien le admitira con los\n",
            "CER: 0.169, WER: 0.385\n",
            "\n",
            "Filename: Mendo transcription_page_22.jpg\n",
            "Ground Truth: agrados, que tienen a su Magestad merecidos sus grandes, y conti-\n",
            "Predicted   : agqieos qireirenen a su magestad merecidos sus grandes y conti\n",
            "CER: 0.185, WER: 0.545\n",
            "\n",
            "Filename: Mendo transcription_page_23.jpg\n",
            "Ground Truth: nuados Servicios. Como si no vuiera V.S.I. heredado de sus excellenti-\n",
            "Predicted   : r saese s qts teme hea ae o edte t sedrc reeoai srli\n",
            "CER: 0.757, WER: 1.182\n",
            "\n",
            "Filename: Mendo transcription_page_24.jpg\n",
            "Ground Truth: simos Progenitores ser Guzman el Bueno, con sus acciones ha gran-\n",
            "Predicted   : simos oroitores ser guzman el bueno con sus acciones ha gran\n",
            "CER: 0.123, WER: 0.364\n",
            "\n",
            "Filename: Mendo transcription_page_26.jpg\n",
            "Ground Truth: PDF p2\n",
            "Predicted   : pdf p2\n",
            "CER: 0.500, WER: 0.500\n",
            "\n",
            "Filename: Mendo transcription_page_27.jpg\n",
            "Ground Truth: geado el serlo; logrando dignamente en nuestro Monarcha la gracia:\n",
            "Predicted   : geado o serlo logrando dignamente en nuestro monarcha la gracia\n",
            "CER: 0.076, WER: 0.400\n",
            "\n",
            "Filename: Mendo transcription_page_28.jpg\n",
            "Ground Truth: en su Corte el cariño: en el Orbe todo la estimacion, y la fama en la\n",
            "Predicted   : ez conia alho fatctodneiarc eleduteor ns re o oshohna ot a\n",
            "CER: 0.739, WER: 1.000\n",
            "\n",
            "Filename: Mendo transcription_page_29.jpg\n",
            "Ground Truth: posteridad. El que es comun amparo, no se negará a serlo desta Obra:\n",
            "Predicted   : asoso eq andse eom oa dousr edrerotaenmrha a rcn noa\n",
            "CER: 0.721, WER: 0.923\n",
            "\n",
            "Filename: Mendo transcription_page_3.jpg\n",
            "Ground Truth: AL IllVUSTRISSIMO SEÑOR\n",
            "Predicted   : al illvustrissimo señor\n",
            "CER: 0.826, WER: 1.000\n",
            "\n",
            "Filename: Mendo transcription_page_30.jpg\n",
            "Ground Truth: calificandola con leerla: honrandola con admitirla: y patrocinandola\n",
            "Predicted   : s ads lesa edotonr b h lninbte aoama iliodt o oa ndayanat cs\n",
            "CER: 0.765, WER: 1.625\n",
            "\n",
            "Filename: Mendo transcription_page_31.jpg\n",
            "Ground Truth: con repetir en nombre de su Autor a su Magestad este obsequio, que\n",
            "Predicted   : e e r tsms de e drs rdostd me eqroipdiodeteoats et ts nco\n",
            "CER: 0.758, WER: 0.923\n",
            "\n",
            "Filename: Mendo transcription_page_32.jpg\n",
            "Ground Truth: siendo Idea de vn Principe Perfecto, le retrata al viuo, como tam-\n",
            "Predicted   : siendo ide e vn principe perfecto le retrata al viuo como tam\n",
            "CER: 0.121, WER: 0.500\n",
            "\n",
            "Filename: Mendo transcription_page_33.jpg\n",
            "Ground Truth: bien a V.S.I. siendo espejo de vn Ministro ajustado. Cuya vida pro-\n",
            "Predicted   : bien i vsi siendo espejo de vn ministro ajustado cuya vida pro\n",
            "CER: 0.164, WER: 0.500\n",
            "\n",
            "Filename: Mendo transcription_page_34.jpg\n",
            "Ground Truth: spere el cielo en la mayor grandeza para el bien, y felicidad publica.\n",
            "Predicted   : qo aea aeioangeyi b es seo tecad ta orasco caeniamria\n",
            "CER: 0.743, WER: 1.000\n",
            "\n",
            "Filename: Mendo transcription_page_35.jpg\n",
            "Ground Truth: Salamanca Nouiembre 15. de 1659.\n",
            "Predicted   : salamanca nouiembre 15 de 1659\n",
            "CER: 0.125, WER: 0.800\n",
            "\n",
            "Filename: Mendo transcription_page_37.jpg\n",
            "Ground Truth: Humilde Capellan de V.S.I\n",
            "Predicted   : humilde capellan de vsi\n",
            "CER: 0.280, WER: 0.750\n",
            "\n",
            "Filename: Mendo transcription_page_38.jpg\n",
            "Ground Truth: que su mano besa.\n",
            "Predicted   : qe su mano besa\n",
            "CER: 0.118, WER: 0.500\n",
            "\n",
            "Filename: Mendo transcription_page_39.jpg\n",
            "Ground Truth: ANDRES MENDO.\n",
            "Predicted   : anddres mendo\n",
            "CER: 1.000, WER: 1.000\n",
            "\n",
            "Filename: Mendo transcription_page_4.jpg\n",
            "Ground Truth: DON ALONSO PEREZ\n",
            "Predicted   : don alonso perez\n",
            "CER: 0.875, WER: 1.000\n",
            "\n",
            "Filename: Mendo transcription_page_41.jpg\n",
            "Ground Truth: PDF p3\n",
            "Predicted   : pdf p2\n",
            "CER: 0.667, WER: 1.000\n",
            "\n",
            "Filename: Mendo transcription_page_42.jpg\n",
            "Ground Truth: APROBACION\n",
            "Predicted   : aprobacion\n",
            "CER: 1.000, WER: 1.000\n",
            "\n",
            "Filename: Mendo transcription_page_43.jpg\n",
            "Ground Truth: Del Doctor D. Francisco de Puga, y Feijoo, Colegial del\n",
            "Predicted   : del doctor d francisco de puga y feijoo colegial del\n",
            "CER: 0.182, WER: 0.700\n",
            "\n",
            "Filename: Mendo transcription_page_44.jpg\n",
            "Ground Truth: Colegio Mayor del Arzobispo de Toledo, y Cath-\n",
            "Predicted   : colegio moyor del arzobispo de toledo y cath\n",
            "CER: 0.174, WER: 0.625\n",
            "\n",
            "Filename: Mendo transcription_page_45.jpg\n",
            "Ground Truth: dratico de Prima de Canones de la Vni-\n",
            "Predicted   : dratico de prima de canones de la vni\n",
            "CER: 0.105, WER: 0.375\n",
            "\n",
            "Filename: Mendo transcription_page_46.jpg\n",
            "Ground Truth: versidad de Salamanca.\n",
            "Predicted   : versid de salamanca\n",
            "CER: 0.182, WER: 0.667\n",
            "\n",
            "Filename: Mendo transcription_page_48.jpg\n",
            "Ground Truth: Por Comision del Illustrissimo Señor Don\n",
            "Predicted   : por comision del illustrissimo señor don\n",
            "CER: 0.125, WER: 0.833\n",
            "\n",
            "Filename: Mendo transcription_page_49.jpg\n",
            "Ground Truth: Iuan Perez Delgado Obispo de Salamanca,\n",
            "Predicted   : iuan pere delgado obispo de salamanca\n",
            "CER: 0.179, WER: 0.833\n",
            "\n",
            "Filename: Mendo transcription_page_5.jpg\n",
            "Ground Truth: DE GVZMAN EL BUENO,\n",
            "Predicted   : de gvzman el bueno\n",
            "CER: 0.842, WER: 1.000\n",
            "\n",
            "Filename: Mendo transcription_page_50.jpg\n",
            "Ground Truth: del Consejo de su Magestad; He leydo con\n",
            "Predicted   : de consejo de su magestad he leydo con\n",
            "CER: 0.125, WER: 0.500\n",
            "\n",
            "Filename: Mendo transcription_page_51.jpg\n",
            "Ground Truth: atencion este libro de Documentos Politi-\n",
            "Predicted   : atencion este libro de documentos politi\n",
            "CER: 0.073, WER: 0.333\n",
            "\n",
            "Filename: Mendo transcription_page_52.jpg\n",
            "Ground Truth: cos, y Morales para vn Principe PErfecto, y Ministros\n",
            "Predicted   : cos y morales para vn principe perfecto y ministros\n",
            "CER: 0.132, WER: 0.556\n",
            "\n",
            "Filename: Mendo transcription_page_53.jpg\n",
            "Ground Truth: ajustados, que escribió nuestro muy Reverendo Padre\n",
            "Predicted   : ajstaos que escribió nuestro muy reverendo padre\n",
            "CER: 0.098, WER: 0.429\n",
            "\n",
            "Filename: Mendo transcription_page_54.jpg\n",
            "Ground Truth: Andres Mendo de la Comañia de IESVS, Lector que\n",
            "Predicted   : andres mend de la comañia de iesvs lector que\n",
            "CER: 0.234, WER: 0.556\n",
            "\n",
            "Filename: Mendo transcription_page_55.jpg\n",
            "Ground Truth: fue aqui de Theologia, y Escritura, Rector del Colegio\n",
            "Predicted   : fue aqui de theologia y escritura rector del colegio\n",
            "CER: 0.111, WER: 0.444\n",
            "\n",
            "Filename: Mendo transcription_page_56.jpg\n",
            "Ground Truth: de Irlandeses desta Vniversidad, y Calificado del Con-\n",
            "Predicted   : de irlandeses desta vniversidad y calificado del con\n",
            "CER: 0.111, WER: 0.500\n",
            "\n",
            "Filename: Mendo transcription_page_57.jpg\n",
            "Ground Truth: sejo de la Inquisicion Suprema. Confieso mi dicha, por\n",
            "Predicted   : sejo de la nquisicion suprema confieso mi dicha por\n",
            "CER: 0.093, WER: 0.444\n",
            "\n",
            "Filename: Mendo transcription_page_58.jpg\n",
            "Ground Truth: auerla tenido en participar noticias de tanta erudcion\n",
            "Predicted   : au eraa tenido en participar noticias de tanta erudcion\n",
            "CER: 0.037, WER: 0.250\n",
            "\n",
            "Filename: Mendo transcription_page_59.jpg\n",
            "Ground Truth: asi Sagrada, como Profana, antes que la estampa las co-\n",
            "Predicted   : asi sagraa como profana antes que la estampa las co\n",
            "CER: 0.109, WER: 0.300\n",
            "\n",
            "Filename: Mendo transcription_page_6.jpg\n",
            "Ground Truth: PATRIARCHA DE LAS INDIAS\n",
            "Predicted   : patriarcha de las indias\n",
            "CER: 0.875, WER: 1.000\n",
            "\n",
            "Filename: Mendo transcription_page_60.jpg\n",
            "Ground Truth: municarse a todos; bienque nunca pueden hazerse vul-\n",
            "Predicted   : muri carse ra todos bienque nunca pueden hazerse vul\n",
            "CER: 0.096, WER: 0.625\n",
            "\n",
            "Filename: Mendo transcription_page_61.jpg\n",
            "Ground Truth: gares, por mas que se solicite su vtilidad. Al Autor le han\n",
            "Predicted   : ges por mas que se solicite su vtilidad al autor le han\n",
            "CER: 0.102, WER: 0.333\n",
            "\n",
            "Filename: Mendo transcription_page_62.jpg\n",
            "Ground Truth: puesto sus Escritos en la primera Clase destos tiempo,\n",
            "Predicted   : pueste sus escritos en la primera clase destos tiempo\n",
            "CER: 0.074, WER: 0.444\n",
            "\n",
            "Filename: Mendo transcription_page_63.jpg\n",
            "Ground Truth: sin que este lugar pueda controvertirsele la desatencion\n",
            "Predicted   : sin qie este lugar pueda controvertirsele la desatencion\n",
            "CER: 0.018, WER: 0.125\n",
            "\n",
            "Filename: Mendo transcription_page_64.jpg\n",
            "Ground Truth: mas injusta. No hay empeño de letras, a que no satisfaga\n",
            "Predicted   : ms unjuta no hay empeño de letras a que no satisfaga\n",
            "CER: 0.107, WER: 0.364\n",
            "\n",
            "Filename: Mendo transcription_page_65.jpg\n",
            "Ground Truth: con su caudal ventajoso. Lo extraordinario, lo raro, se lo\n",
            "Predicted   : can su caua ventajoso lo extraordinario lo raro se lo\n",
            "CER: 0.121, WER: 0.600\n",
            "\n",
            "Filename: Mendo transcription_page_66.jpg\n",
            "Ground Truth: reservó la providencia con tanto acierto, que la senda\n",
            "Predicted   : reserveva providencia con tanto acierto que la senda\n",
            "CER: 0.074, WER: 0.333\n",
            "\n",
            "Filename: Mendo transcription_page_67.jpg\n",
            "Ground Truth: mas estrecha la haze comino Real a la tarea incesable\n",
            "Predicted   : mas eatreaha la haze comino real a la tarea incesable\n",
            "CER: 0.057, WER: 0.200\n",
            "\n",
            "Filename: Mendo transcription_page_68.jpg\n",
            "Ground Truth: de su estudiosidad. No ay rumbo por descubrir al norte\n",
            "Predicted   : de su estudiosidad no ay rumbo por descubrir al norte\n",
            "CER: 0.037, WER: 0.200\n",
            "\n",
            "Filename: Mendo transcription_page_69.jpg\n",
            "Ground Truth: de su ingenio, ni aspereza por facilitar a su huella. Señas\n",
            "Predicted   : de isisngenio ni aspereza por facilitar a su huella señas\n",
            "CER: 0.119, WER: 0.364\n",
            "\n",
            "Filename: Mendo transcription_page_7.jpg\n",
            "Ground Truth: Arzobispo de Tyro, Limosnero mayor del Rey\n",
            "Predicted   : arzobisipo de tyro limosnero mayor del rey\n",
            "CER: 0.143, WER: 0.571\n",
            "\n",
            "Filename: Mendo transcription_page_70.jpg\n",
            "Ground Truth: son estas de sus libros de la Exposicion de la Bulla; De Iure\n",
            "Predicted   : son estas de sus libros de la exposicion de la bulla de iure\n",
            "CER: 0.082, WER: 0.308\n",
            "\n",
            "Filename: Mendo transcription_page_71.jpg\n",
            "Ground Truth: Academico; De Ordinibus Militaribus; Obras mas alla del\n",
            "Predicted   : academico de ordinibus militaribus obras mas alla del\n",
            "CER: 0.127, WER: 0.625\n",
            "\n",
            "Filename: Mendo transcription_page_72.jpg\n",
            "Ground Truth: aplauso mas encarecido. En esta el metodo la asegura de\n",
            "Predicted   : apleso mas encarecido en esta el metodo la asegura de\n",
            "CER: 0.073, WER: 0.300\n",
            "\n",
            "Filename: Mendo transcription_page_73.jpg\n",
            "Ground Truth: Censura, distribuyendo las Virtudes morales de vn Prin-\n",
            "Predicted   : censera destribuyendo las virtudes morales de vn prin\n",
            "CER: 0.127, WER: 0.500\n",
            "\n",
            "Filename: Mendo transcription_page_74.jpg\n",
            "Ground Truth: cipe con ilacion; que hasta en la variedad suele echarse\n",
            "Predicted   : ci in con i acion que hasta en la variedad suele echarse\n",
            "CER: 0.089, WER: 0.400\n",
            "\n",
            "Filename: Mendo transcription_page_75.jpg\n",
            "Ground Truth: menos la consequencia. El estilo le proporciona a la se-\n",
            "Predicted   : msno la onsequencia el estilo le proporciona a la se\n",
            "CER: 0.107, WER: 0.400\n",
            "\n",
            "Filename: Mendo transcription_page_77.jpg\n",
            "Ground Truth: PDF p4\n",
            "Predicted   : pdf p2\n",
            "CER: 0.667, WER: 1.000\n",
            "\n",
            "Filename: Mendo transcription_page_78.jpg\n",
            "Ground Truth: eriedad del asumpto, vsaudole grave, y ponderoso, sin que\n",
            "Predicted   : eriedead de asumpto vsaudole grave y ponderoso sin que\n",
            "CER: 0.088, WER: 0.556\n",
            "\n",
            "Filename: Mendo transcription_page_79.jpg\n",
            "Ground Truth: le encumbre la afectacion; Que es alaja muy estimable\n",
            "Predicted   : lem encumre la afectacion que es alaja muy estimable\n",
            "CER: 0.075, WER: 0.444\n",
            "\n",
            "Filename: Mendo transcription_page_8.jpg\n",
            "Ground Truth: Nuestro Señor Don Felipe IV. El Grande Rey de\n",
            "Predicted   : nuestro señor don felipe iv el grande rey de\n",
            "CER: 0.222, WER: 0.889\n",
            "\n",
            "Filename: Mendo transcription_page_80.jpg\n",
            "Ground Truth: de la prudencia, que la pluma por si mas remontada qo-\n",
            "Predicted   : de la pruaua ia que la pluma por si mas remontada qo\n",
            "CER: 0.111, WER: 0.273\n",
            "\n",
            "Filename: Mendo transcription_page_81.jpg\n",
            "Ground Truth: uierne sus buelos al peso de las materias. Quantos co-\n",
            "Predicted   : uerne sus buelos al peso de las materias quantos co\n",
            "CER: 0.074, WER: 0.400\n",
            "\n",
            "Filename: Mendo transcription_page_82.jpg\n",
            "Ground Truth: nocen al Autor, le han oydo frequentemente en los pul-\n",
            "Predicted   : noaen aautor le han oydo frequentemente en los pul\n",
            "CER: 0.111, WER: 0.400\n",
            "\n",
            "Filename: Mendo transcription_page_83.jpg\n",
            "Ground Truth: pitos, y le han venerado con el primer credito en este\n",
            "Predicted   : pitos o le han venerado con el primer credito en este\n",
            "CER: 0.037, WER: 0.182\n",
            "\n",
            "Filename: Mendo transcription_page_84.jpg\n",
            "Ground Truth: exercicio, con que saben la linea, a que llega su eloquen-\n",
            "Predicted   : eierocicro con que saben la linea a que llega su eloquen\n",
            "CER: 0.103, WER: 0.273\n",
            "\n",
            "Filename: Mendo transcription_page_85.jpg\n",
            "Ground Truth: cia, nunca inferior a la de Tullio, y Demosthenes. La vti-\n",
            "Predicted   : cia nuncainferior a la de tullio y demosthenes la vti\n",
            "CER: 0.138, WER: 0.636\n",
            "\n",
            "Filename: Mendo transcription_page_86.jpg\n",
            "Ground Truth: lidad del libre se conoce del fin, que por el se pretende.\n",
            "Predicted   : lida dele libre se conoce del fin que por el se pretende\n",
            "CER: 0.069, WER: 0.333\n",
            "\n",
            "Filename: Mendo transcription_page_87.jpg\n",
            "Ground Truth: Su enseñanza es la Idea de vn Principe Perfecto, y siend-\n",
            "Predicted   : su enseñanza es la idea de vn principe perfecto y siend\n",
            "CER: 0.105, WER: 0.455\n",
            "\n",
            "Filename: Mendo transcription_page_88.jpg\n",
            "Ground Truth: do este o alma, o cabeza del cuerpo de la Republica,\n",
            "Predicted   : do este o alma o cabeza del cuerpo de la republica\n",
            "CER: 0.058, WER: 0.182\n",
            "\n",
            "Filename: Mendo transcription_page_89.jpg\n",
            "Ground Truth: bien se deja entender, quan benignas influencias causara\n",
            "Predicted   : ben se sejaentender quan benignas influencias causara\n",
            "CER: 0.071, WER: 0.375\n",
            "\n",
            "Filename: Mendo transcription_page_9.jpg\n",
            "Ground Truth: las Españas, del Consejo de su Magestad, y Iuez\n",
            "Predicted   : las españas del consejo de su magestad y iuez\n",
            "CER: 0.128, WER: 0.444\n",
            "\n",
            "Filename: Mendo transcription_page_90.jpg\n",
            "Ground Truth: su virtud en las costumbres de sus Vasallos, y quanto\n",
            "Predicted   : s virtud edn las costumbres de sus vasallos y quanto\n",
            "CER: 0.075, WER: 0.300\n",
            "\n",
            "Filename: Mendo transcription_page_91.jpg\n",
            "Ground Truth: mejor obrarán estos advertidos de su exemplo. Con que\n",
            "Predicted   : mejor obrarn estos advertidos de su exemplo con que\n",
            "CER: 0.057, WER: 0.333\n",
            "\n",
            "Filename: Mendo transcription_page_92.jpg\n",
            "Ground Truth: hallarán en esta obra los Principes, que aprender, y los\n",
            "Predicted   : hallar en esta obra los principes que aprender y los\n",
            "CER: 0.089, WER: 0.300\n",
            "\n",
            "Filename: Mendo transcription_page_93.jpg\n",
            "Ground Truth: Vasallos, que imitar, y todos quanta enseñanza Christia-\n",
            "Predicted   : vasllos que imitar y todos quanta enseñanza christia\n",
            "CER: 0.107, WER: 0.375\n",
            "\n",
            "Filename: Mendo transcription_page_94.jpg\n",
            "Ground Truth: na, y Politica conduce, para saber governar, y obedecer.\n",
            "Predicted   : na y oatica conduce para saber governar y obedecer\n",
            "CER: 0.125, WER: 0.556\n",
            "\n",
            "Filename: Mendo transcription_page_95.jpg\n",
            "Ground Truth: Ardua empresa es, animar los ojos a esfera tan Superior,\n",
            "Predicted   : areuau es presa es animar los ojos a esfera tan superior\n",
            "CER: 0.143, WER: 0.500\n",
            "\n",
            "Filename: Mendo transcription_page_96.jpg\n",
            "Ground Truth: como la de vn Monarca, en que suele el sol de la gran-\n",
            "Predicted   : como  de vn monarca en que suele el sol de la gran\n",
            "CER: 0.093, WER: 0.231\n",
            "\n",
            "Filename: Mendo transcription_page_97.jpg\n",
            "Ground Truth: deza dejar desmayada la mas perspicaz vista; pero co-\n",
            "Predicted   : deaadea desmayada la mas perspicaz vista pero co\n",
            "CER: 0.113, WER: 0.444\n",
            "\n",
            "Filename: Mendo transcription_page_98.jpg\n",
            "Ground Truth: mo le idea perfecto el Autor, son mas serenas, que rigu-\n",
            "Predicted   : mo le ee perfecto el autor son mas serenas que rigu\n",
            "CER: 0.125, WER: 0.364\n",
            "\n",
            "Filename: Mendo transcription_page_99.jpg\n",
            "Ground Truth: rosas sus luces. Existimare quidem de Principe, nemini\n",
            "Predicted   : rlosasl sus luces existimare quidem de principe nemini\n",
            "CER: 0.111, WER: 0.500\n",
            "\n",
            "Filename: PORCONES.228.35 1636 transcription_page_1.jpg\n",
            "Ground Truth: NOTES:\t\tu and v are used interchangeably \tcheck against dictionary?\n",
            "\t\ttwo types of lowercase “s” -> ‘s’ and ‘ſ’  both should be transcribed as ‘s’\n",
            "\t\taccents are inconsistent \t\tshould be ignored (except ñ)\n",
            "\t\tsome letters have macrons (¯)\t\ttends to mean ‘n’ follows, or ‘ue’ after capped q\n",
            "\t\tsome line end hyphens not present\tleave words split for now, can decide later\n",
            "\t\tç old spelling is always modern z\tteach AI to always interpret ç as z\n",
            "Predicted   : ntetdtetestnar c a d di rc s sleaureererio\n",
            "CER: 0.913, WER: 1.000\n",
            "\n",
            "Filename: PORCONES.228.35 1636 transcription_page_10.jpg\n",
            "Ground Truth: END OF EXTRACT\n",
            "Predicted   : end of etract\n",
            "CER: 0.857, WER: 1.000\n",
            "\n",
            "Filename: PORCONES.228.35 1636 transcription_page_2.jpg\n",
            "Ground Truth: PDF p1\n",
            "Predicted   : pdf p2\n",
            "CER: 0.667, WER: 1.000\n",
            "\n",
            "Filename: PORCONES.228.35 1636 transcription_page_3.jpg\n",
            "Ground Truth: POR\n",
            "DOÑA FRANCISCA DE\n",
            "Mendoza, y doña Ana de Gue-\n",
            "vara su hija\n",
            "Predicted   : pordoña franisca demendoza y doña ana de guevara su hija\n",
            "CER: 0.419, WER: 0.615\n",
            "\n",
            "Filename: PORCONES.228.35 1636 transcription_page_4.jpg\n",
            "Ground Truth: CON\n",
            "El Conde de Sastago, y \n",
            "Fuen-Clara\n",
            "Predicted   : conelnde de sastago y fuenclara\n",
            "CER: 0.368, WER: 0.714\n",
            "\n",
            "Filename: PORCONES.228.35 1636 transcription_page_5.jpg\n",
            "Ground Truth: Pretende doña Ana, y D. Francisca, que se ha de\n",
            "emendar la sentencia de que esta suplicado, y \n",
            "que ha de ser condenado el Conde de Sastago \n",
            "en la pena de muerte en que ha incurrido, por\n",
            "los delitos de estupro, y quebrantamiento de ca\n",
            "sa, de que es acusado. \n",
            "Para lo qual se supone por hecho constante, que aviendo galan\n",
            "teado por el año de treinta y uno el Conde de sastago, siendo\n",
            "Conde de Fuenclara, doña Ana de Guevara, y estupradola deba\n",
            "xo de palabra de casamiento, y tenido en ella por sus hijos a\n",
            "don Martin, y a doña Ana de Alagon, se ausento destos Rey-\n",
            "Predicted   : cdotaniotusni are a  reyra ernyr ttsussmeba\n",
            "CER: 0.929, WER: 0.991\n",
            "\n",
            "Filename: PORCONES.228.35 1636 transcription_page_6.jpg\n",
            "Ground Truth: PDF p2\n",
            "Predicted   : pdf p2\n",
            "CER: 0.500, WER: 0.500\n",
            "\n",
            "Filename: PORCONES.228.35 1636 transcription_page_7.jpg\n",
            "Ground Truth: nos a los Estados de Flandes, sin cumplir la dicha palabra, por\n",
            "dezir que no tenia comodidad para casarse, hasta q su Magestad\n",
            "le hiziesse merced: y aviendo buelto a estos Reynos por pin-\n",
            "cipio de Julio del año de treinta y seis, faltando al cumplimien\n",
            "to de la dicha palabra, en que avia hecho instancia doña Ana\n",
            "de Guevara desde que sucedió el caso, trato de casarse en Pala-\n",
            "cio con la hija de la Condesa de Salvatierra, y aviendo llegado\n",
            "a noticia de doña Ana de Guevara, a los quinze dias del dicho\n",
            "mes de Julio del año de treinta y seis, postrada a los Reales pies\n",
            "de su Magestad, pidio que la hiziesse justicia contra el Conde de\n",
            "Sastago, representandole su agravio, y la palabra que la avia da-\n",
            "do, y prendas, que debaxo della estavan clamando por la justi-\n",
            "cia que pedia: y aviendo su magestad formado una Junta de los\n",
            "mas graves Ministros desta Corte, para que le consultassen lo q\n",
            "devia hazer, cometio la averiguacion deste caso al señor D. Fran\n",
            "cisto Antonio de Alarcon, uno de los Ministros della, el qual por \n",
            "su persona examino cinco testigos, que dixeron contestes de la\n",
            "palabra de matrimonio del estrupo, del quebrantamiento de la\n",
            "casa, y de los dos partos, de don Martin, y de doña Ana de Ala-\n",
            "gon y Pimentel, que el Conde ha reconocido por su shijos, por\n",
            "los instrumentos publicos q estan presentados, confessando tam-\n",
            "bien en ellos, que fueron avidos en persona igual con el en la ca\n",
            "lidad.\n",
            "Y cuando se avia de ir prosiguiendo en la causa, aviendo lle-\n",
            "gado a noticia del Conde, como se avia formado la dicha Junta, \n",
            "y que doña Ana de Guevara avia puesto en manos de su Mag.\n",
            "su agravio, salio a mucha prisa desta Corte, y a toda diligencia\n",
            "se bolvio a los dichos Estados, y por consulta de la Junta, su Ma\n",
            "gestad remitio diferentes decretos al señor Infante Cardenal, y\n",
            "despues de su muerte a don Francisco Melo, y al Marques de\n",
            "Castelrodrigo, para que le remitiessen a esta Corte, y viniesse a\n",
            "estar a derecho con doña Ana de Guevara, y con la mucha ma-\n",
            "no que tenia en los dichos Estados, y puestos que ocupava, se di\n",
            "lato esta venida, hasta el año passado de quarenta y seis, que avien\n",
            "do ido mas apretados decretos de su Mag. Vino a esta Corte. \n",
            "Y aviendo su Mag. Mandado formar otra Junta de los señores\n",
            "D. Francisco de Robles, D. Francisco de Solis, D. Pedro de Velas-\n",
            "co, y D. Bernardo de Peñarrieta, D. Francisca de Mendoza, y D.\n",
            "Predicted   : te   to\n",
            "CER: 0.997, WER: 0.998\n",
            "\n",
            "Filename: PORCONES.228.35 1636 transcription_page_8.jpg\n",
            "Ground Truth: PDF p3\n",
            "Predicted   : pdf p2\n",
            "CER: 0.667, WER: 1.000\n",
            "\n",
            "Filename: PORCONES.228.35 1636 transcription_page_9.jpg\n",
            "Ground Truth: Ana de Guevara, prosiguiendo la queja, querella, y agravio q avia\n",
            "representado a su Magestad, se querellaron criminalmente del \n",
            "Conde, por el estrupo y quebrantamiento, y pidieron que\n",
            "fuesse condenado en las mayores penas en que avia incurrido.\n",
            "Y ante el Juez Eclesiástico doña Ana prosiguio la demanda q\n",
            "avia propuesto quando estava en los Estados de Flandes, de que\n",
            "se avia despachado requisitoria, cuayo cumplimiento se dilato\n",
            "en la misma forma, donde obtuvo sentencia, en que fue conde-\n",
            "nado el Conde al cumplimiento de la palabra: y aviendose pre-\n",
            "sentado en la Junta testimonio della, concluso el pleyto la pro-\n",
            "nuncio sentencia, condenando al Conde en seis años del presidio\n",
            "de Oran, con diez lanzas a su costa, y en treinta mil ducados para\n",
            "doña Ana de Guevara, y en quatro mil ducados para la Camara \n",
            "de du Mag. En defecto de no casarse con ella. \n",
            "De que doña Francisca, y D. Ana interpusieron suplicacion, \n",
            "pidiendo que fuesse condenado en la pena de muerte, en q avia\n",
            "incurrido, que es sola la pretension que tienen.\n",
            "Y estando concluso el pleyto en revista en la dicha Real Jun-\n",
            "ta, por particular decreto de su Ma. Ganado a pedimiento del\n",
            "Conde, se mando que lo viesse todo el Consejo, de quien ambas\n",
            "esperan la emienda de la dicha sentencia que piden, ex seqq.\n",
            "Lo primero, porque esta causa, como las demas criminales, \n",
            "tiene tres puntos.\n",
            "El primero, ver y examinar la gravedad de los delitos.\n",
            "El segundo, si estan probados.\n",
            "Y el tercero, que penas les corresponden.\n",
            "Y en quanto al primero, para ponderar su gravedad, es neces-\n",
            "sario ponderar la calidad de las personas, ut ait consultum in l.fin.\n",
            "ff.de actionib.&oblig.ibi: Ex personis,causis que iudicen astimatu-\n",
            "rum, an actio danda fit.I. Pedius, &, Diuus Pius, ff. De inced. Ruina, \n",
            "& nanfrag.ibi: Et omnino ut in cateris, it a in huius modi causis ex\n",
            "personarum conditione, & rerum qualitate diligenter poena sunt\n",
            "astimada, ne quid aut durius, aut remisius constituatur quam cau\n",
            "sa postulabit, c.Pastoralis.28.in princ.ibi: Dignitati defferat, &\n",
            "persona de offic.& potest.iud.dileg.I.si crimen 3. De ordine cognitio\n",
            "num.1.inferuor.10.ff.de poenis.\n",
            "Y considerada la persona de D. Ana de Guevara, en quanto al \n",
            "estupro, viene a ser atrocissimo delito, por ser como es hija de la\n",
            "Predicted   : tu   te\n",
            "CER: 0.997, WER: 1.000\n",
            "\n",
            "Filename: Paredes transcription_page_1.jpg\n",
            "Ground Truth: NOTES:\t\tu and v are used interchangeably \tcheck against dictionary?\n",
            "\t\ttwo types of lowercase “s” -> ‘s’ and ‘ſ’  both should be transcribed as ‘s’\n",
            "\t\taccents are inconsistent \t\tshould be ignored (except ñ)\n",
            "\t\tsome letters have macrons (¯)\t\tshould mean ‘n’ follows, or ‘ue’ after capped q\n",
            "\t\tsome line end hyphens not present\tleave words split for now, can decide later\n",
            "\t\tç old spelling is always modern z\tteach AI to always interpret ç as z\n",
            "Predicted   : ntetdtetestni r cq oar d d si tsleureereas\n",
            "CER: 0.908, WER: 1.000\n",
            "\n",
            "Filename: Paredes transcription_page_100.jpg\n",
            "Ground Truth: que imprimen, porque como siempre sirve de consonante, hi-\n",
            "Predicted   : qimeimporque como siempre sirve de consonante hi\n",
            "CER: 0.190, WER: 0.556\n",
            "\n",
            "Filename: Paredes transcription_page_101.jpg\n",
            "Ground Truth: riendo sobre las vocales y la v tiene en muchisimos casos la\n",
            "Predicted   : riendo seroelas vocales y la v tiene en muchisimos casos la\n",
            "CER: 0.067, WER: 0.167\n",
            "\n",
            "Filename: Paredes transcription_page_102.jpg\n",
            "Ground Truth: misma propiedad, le haze dificultoso saber qué palabras se po-\n",
            "Predicted   : mia propiedai le haze dificultoso saber qué palabras se po\n",
            "CER: 0.081, WER: 0.300\n",
            "\n",
            "Filename: Paredes transcription_page_103.jpg\n",
            "Ground Truth: nen con la vna, y quales con la otra letra: para inteligencia de\n",
            "Predicted   : n on on l na y quales con la otra letra para inteligencia de\n",
            "CER: 0.109, WER: 0.462\n",
            "\n",
            "Filename: Paredes transcription_page_104.jpg\n",
            "Ground Truth: lo qual se had de saber, que es regla assentada, que siempre que al\n",
            "Predicted   : qet en en b bosroehs iln eot e ecrutentaonrasn ruses\n",
            "CER: 0.761, WER: 1.000\n",
            "\n",
            "Filename: Paredes transcription_page_105.jpg\n",
            "Ground Truth: pronuncia la palabra se juntan los labios, y abre la boca, para\n",
            "Predicted   : pronanria lanralabra se juntan los labios y abre la boca para\n",
            "CER: 0.095, WER: 0.417\n",
            "\n",
            "Filename: Paredes transcription_page_106.jpg\n",
            "Ground Truth: que la voz salga con fuerza, como bien, bueno, ha de llevar b; y\n",
            "Predicted   : que ae z salga con fuerza como bien bueno ha de llevar b y\n",
            "CER: 0.125, WER: 0.429\n",
            "\n",
            "Filename: Paredes transcription_page_13.jpg\n",
            "Ground Truth: Para los que aprendieren este Arte, y quisieren saber donde se\n",
            "Predicted   : par r s qre aprendieren este arte y quisieren saber donde se\n",
            "CER: 0.113, WER: 0.455\n",
            "\n",
            "Filename: Paredes transcription_page_14.jpg\n",
            "Ground Truth: echa cada letra, me parece que aqui tienen bastante demonstra-\n",
            "Predicted   : echa aetra me parece que aqui tienen bastante demonstra\n",
            "CER: 0.113, WER: 0.300\n",
            "\n",
            "Filename: Paredes transcription_page_15.jpg\n",
            "Ground Truth: cion. A gunos caxoncitos que quedan en b anco, reparten en e os\n",
            "Predicted   : cio gun axoncitos que quedan en b anco reparten en e os\n",
            "CER: 0.127, WER: 0.385\n",
            "\n",
            "Filename: Paredes transcription_page_16.jpg\n",
            "Ground Truth: los estrangeros algunas letras igadas de que vsan mas que noso-\n",
            "Predicted   : lorastrors algunas letras igadas de que vsan mas que noso\n",
            "CER: 0.143, WER: 0.273\n",
            "\n",
            "Filename: Paredes transcription_page_17.jpg\n",
            "Ground Truth: tros, y a gunos caracteres diferentes, necesarios para la pronun-\n",
            "Predicted   : trs a sr s caracteres diferentes necesarios para la pronun\n",
            "CER: 0.154, WER: 0.600\n",
            "\n",
            "Filename: Paredes transcription_page_18.jpg\n",
            "Ground Truth: ciacion de sus lenguas so amente, y no para nuestra.\n",
            "Predicted   : c acio i si lenguas so amente y no para nuestra\n",
            "CER: 0.154, WER: 0.600\n",
            "\n",
            "Filename: Paredes transcription_page_19.jpg\n",
            "Ground Truth: Lo primero en que debe exercitarse e Impressor, es en distri-\n",
            "Predicted   : loeimero e que debe exercitarse e impressor es en distri\n",
            "CER: 0.131, WER: 0.455\n",
            "\n",
            "Filename: Paredes transcription_page_20.jpg\n",
            "Ground Truth: buir, para lo qual es necessario no tomar grande la tomada, y esta\n",
            "Predicted   : te aeatantos erun es m e aoe aretsnester ven qiatera lcs\n",
            "CER: 0.727, WER: 0.923\n",
            "\n",
            "Filename: Paredes transcription_page_21.jpg\n",
            "Ground Truth: con su reg eta poner a encima de la palma, y quatro dedos de la\n",
            "Predicted   : cs gs reg eta poner a encima de la palma y quatro dedos de la\n",
            "CER: 0.079, WER: 0.200\n",
            "\n",
            "Filename: Paredes transcription_page_22.jpg\n",
            "Ground Truth: mano izquierda, afiianzada con el dedo pu gar por os principios\n",
            "Predicted   : mano izquieafiianzada con el dedo pu gar por os principios\n",
            "CER: 0.079, WER: 0.182\n",
            "\n",
            "Filename: Paredes transcription_page_23.jpg\n",
            "Ground Truth: de los reng ones. Despues  con la mano derecha tomar vn , o dos\n",
            "Predicted   : de los eng e es despues  con la mano derecha tomar vn  o dos\n",
            "CER: 0.095, WER: 0.357\n",
            "\n",
            "Filename: Paredes transcription_page_24.jpg\n",
            "Ground Truth: palabras, con los dedos pu gar, indice, y del corazon, y q descansen\n",
            "Predicted   : pa abrao coa ras dedos pu gar indice y del corazon y q descansen\n",
            "CER: 0.132, WER: 0.538\n",
            "\n",
            "Filename: Paredes transcription_page_26.jpg\n",
            "Ground Truth: f. 9r (PDF p. 2)\n",
            "Predicted   : f 9r pdf p 2\n",
            "CER: 0.438, WER: 0.800\n",
            "\n",
            "Filename: Paredes transcription_page_28.jpg\n",
            "Ground Truth: en el dedo quarto de la mano: luego en lyeyendo lo que ha toma-\n",
            "Predicted   : en l ededqdqarto de la mano luego en lyeyendo lo que ha toma\n",
            "CER: 0.111, WER: 0.357\n",
            "\n",
            "Filename: Paredes transcription_page_29.jpg\n",
            "Ground Truth: do, ir con los dos dedos pulgar, y indice echando cada etra en su\n",
            "Predicted   : do ir on l n os dedos pulgar y indice echando cada etra en su\n",
            "CER: 0.092, WER: 0.429\n",
            "\n",
            "Filename: Paredes transcription_page_3.jpg\n",
            "Ground Truth: f. 9v (PDF p. 1)\n",
            "Predicted   : f 9v pdf p 1\n",
            "CER: 0.438, WER: 0.800\n",
            "\n",
            "Filename: Paredes transcription_page_30.jpg\n",
            "Ground Truth: caxoncito, poniendo mucho cuidado en no errarlos, para tener\n",
            "Predicted   : caioncito apiniendo mucho cuidado en no errarlos para tener\n",
            "CER: 0.083, WER: 0.333\n",
            "\n",
            "Filename: Paredes transcription_page_31.jpg\n",
            "Ground Truth: después poco que corregir. La letra, para que este más tratable, y\n",
            "Predicted   : despopo popo que corregir la letra para que este más tratable y\n",
            "CER: 0.121, WER: 0.500\n",
            "\n",
            "Filename: Paredes transcription_page_32.jpg\n",
            "Ground Truth: se distribuya con más velocidad, ha de estar siempre mojada.\n",
            "Predicted   : se istrir a con más velocidad ha de estar siempre mojada\n",
            "CER: 0.100, WER: 0.400\n",
            "\n",
            "Filename: Paredes transcription_page_33.jpg\n",
            "Ground Truth: Para componer se abraza el componedor con los quatro dedos,\n",
            "Predicted   : pa camponer se abraza el componedor con los quatro dedos\n",
            "CER: 0.085, WER: 0.300\n",
            "\n",
            "Filename: Paredes transcription_page_34.jpg\n",
            "Ground Truth: y palma de la mano izquierda, y el dedo pulgar recibe la letra que\n",
            "Predicted   : p ae a ro sorimeyhao drd lmialutaeo r l ririeain oe co\n",
            "CER: 0.727, WER: 1.000\n",
            "\n",
            "Filename: Paredes transcription_page_35.jpg\n",
            "Ground Truth: la mano derecha trae, y la acomoda como ha de estar: y en qua-\n",
            "Predicted   : la mano dereca trae y la acomoda como ha de estar y en qua\n",
            "CER: 0.065, WER: 0.286\n",
            "\n",
            "Filename: Paredes transcription_page_36.jpg\n",
            "Ground Truth: nto la letra viene desde la caxa al componedor, ha de estar la vista\n",
            "Predicted   : t e o a ai roacaeqcle pes or ebyeedaor d n leatalalas\n",
            "CER: 0.691, WER: 1.000\n",
            "\n",
            "Filename: Paredes transcription_page_37.jpg\n",
            "Ground Truth: en la letra siguiente te que se ha de tomar, para tomarla por la ca-\n",
            "Predicted   : eno o oa agaris l s ro caue citeoa ur r rueoa o olsesnro\n",
            "CER: 0.721, WER: 1.000\n",
            "\n",
            "Filename: Paredes transcription_page_38.jpg\n",
            "Ground Truth: beza, por la mayor brevedad: porque como este exercicio cons\n",
            "Predicted   : be a por reayor brevedad porque como este exercicio cons\n",
            "CER: 0.117, WER: 0.500\n",
            "\n",
            "Filename: Paredes transcription_page_39.jpg\n",
            "Ground Truth: ta de tanto tiempos, qualquiera cosa que se abrevie en cada vno\n",
            "Predicted   : ta t to tiempos qualquiera cosa que se abrevie en cada vno\n",
            "CER: 0.095, WER: 0.250\n",
            "\n",
            "Filename: Paredes transcription_page_40.jpg\n",
            "Ground Truth: haze mucho al fin del dia. Cierto es, que al principio se ha de ir\n",
            "Predicted   : hazc ucho a fin del dia cierto es que al principio se ha de ir\n",
            "CER: 0.091, WER: 0.400\n",
            "\n",
            "Filename: Paredes transcription_page_41.jpg\n",
            "Ground Truth: con espacio, hasta que las manos se vayan habituando: mas en\n",
            "Predicted   : can eoaio hasta que las manos se vayan habituando mas en\n",
            "CER: 0.100, WER: 0.273\n",
            "\n",
            "Filename: Paredes transcription_page_42.jpg\n",
            "Ground Truth: estando habituadas, se hace procurar que con la mano derecha\n",
            "Predicted   : estandoabituadas se hace procurar que con la mano derecha\n",
            "CER: 0.050, WER: 0.200\n",
            "\n",
            "Filename: Paredes transcription_page_43.jpg\n",
            "Ground Truth: se traiga la letra con serenidad, y reposo, sin hacer sonecitos, ni\n",
            "Predicted   : sa ta aleala atra con serenidad y reposo sin hacer sonecitos ni\n",
            "CER: 0.164, WER: 0.583\n",
            "\n",
            "Filename: Paredes transcription_page_44.jpg\n",
            "Ground Truth: movimientos no necessarios, de que algunos tienen grande vi-\n",
            "Predicted   : mnvinto ino necessarios de que algunos tienen grande vi\n",
            "CER: 0.133, WER: 0.444\n",
            "\n",
            "Filename: Paredes transcription_page_45.jpg\n",
            "Ground Truth: cio, que quando quieren no lo pueden remediar, y no serven sino\n",
            "Predicted   : cqn qnue quqd quieren no lo pueden remediar y no serven sino\n",
            "CER: 0.127, WER: 0.333\n",
            "\n",
            "Filename: Paredes transcription_page_46.jpg\n",
            "Ground Truth: de gastar el tiempo sin fruto. En teniendo ya las manos sueltas, y\n",
            "Predicted   : de g star er tiempo sin fruto en teniendo ya las manos sueltas y\n",
            "CER: 0.076, WER: 0.462\n",
            "\n",
            "Filename: Paredes transcription_page_47.jpg\n",
            "Ground Truth: hechas á estos movimientos, se verá con experiencia lo mucho\n",
            "Predicted   : hechas s estos movimientos se verá con experiencia lo mucho\n",
            "CER: 0.033, WER: 0.200\n",
            "\n",
            "Filename: Paredes transcription_page_48.jpg\n",
            "Ground Truth: mas que luce la composicion.\n",
            "Predicted   : ms q luce la composicion\n",
            "CER: 0.143, WER: 0.600\n",
            "\n",
            "Filename: Paredes transcription_page_5.jpg\n",
            "Ground Truth: CAXA ALTA, Y BAXA\n",
            "Predicted   : caa alta y baa\n",
            "CER: 0.824, WER: 1.000\n",
            "\n",
            "Filename: Paredes transcription_page_50.jpg\n",
            "Ground Truth: CAPITVLO TERCERO.\n",
            "Predicted   : capitvlo tercero\n",
            "CER: 0.941, WER: 1.000\n",
            "\n",
            "Filename: Paredes transcription_page_52.jpg\n",
            "Ground Truth: Explicacion de Ortographia, según la doctrina de Felip Mey en el\n",
            "Predicted   : e plicacion de ortographia segn la doctrina de felip mey en el\n",
            "CER: 0.109, WER: 0.545\n",
            "\n",
            "Filename: Paredes transcription_page_53.jpg\n",
            "Ground Truth: Thesaurus verborum, y de Guillelmo Foquel en su Ortographia\n",
            "Predicted   : thesaurus verborum y de guillelmo foquel en su ortographia\n",
            "CER: 0.085, WER: 0.556\n",
            "\n",
            "Filename: Paredes transcription_page_54.jpg\n",
            "Ground Truth: Castellana, y conforme a la correccion que estilava\n",
            "Predicted   : castalla y conforme a la correccion que estilava\n",
            "CER: 0.098, WER: 0.125\n",
            "\n",
            "Filename: Paredes transcription_page_55.jpg\n",
            "Ground Truth: Gonzalo de Ayala.\n",
            "Predicted   : gonzalo de ayala\n",
            "CER: 0.176, WER: 0.667\n",
            "\n",
            "Filename: Paredes transcription_page_57.jpg\n",
            "Ground Truth: Orthographia es palabra Griega, que significa tanto co-\n",
            "Predicted   : orthsirapia es palabra griega que significa tanto co\n",
            "CER: 0.127, WER: 0.375\n",
            "\n",
            "Filename: Paredes transcription_page_58.jpg\n",
            "Ground Truth: mo escritura bien escrita con propiedad, no poniendo\n",
            "Predicted   : mra rocrturi bien escrita con propiedad no poniendo\n",
            "CER: 0.135, WER: 0.375\n",
            "\n",
            "Filename: Paredes transcription_page_59.jpg\n",
            "Ground Truth: letras superfluas, ni dexando de poner las necessarias,\n",
            "Predicted   : leras supefluas ni dexando de poner las necessarias\n",
            "CER: 0.073, WER: 0.375\n",
            "\n",
            "Filename: Paredes transcription_page_6.jpg\n",
            "Ground Truth: Estas están en lugar de versalillas.\n",
            "Predicted   : estas están en lugar de versalillas\n",
            "CER: 0.056, WER: 0.333\n",
            "\n",
            "Filename: Paredes transcription_page_60.jpg\n",
            "Ground Truth: adonandolo con sus apuntuaciones, y acentos, para que bien\n",
            "Predicted   : adondno n sus apuntuaciones y acentos para que bien\n",
            "CER: 0.138, WER: 0.444\n",
            "\n",
            "Filename: Paredes transcription_page_61.jpg\n",
            "Ground Truth: se entienda. Y aunque la letra en la escritura es como el cuerpo,\n",
            "Predicted   : s etie en eunque la letra en la escritura es como el cuerpo\n",
            "CER: 0.154, WER: 0.385\n",
            "\n",
            "Filename: Paredes transcription_page_62.jpg\n",
            "Ground Truth: la Orthographia es alma della.\n",
            "Predicted   : la torthographia es alma della\n",
            "CER: 0.100, WER: 0.400\n",
            "\n",
            "Filename: Paredes transcription_page_63.jpg\n",
            "Ground Truth: Las letras del Abecedario Castellano son venite y tres, aunque\n",
            "Predicted   : las aeras ae abecedario castellano son venite y tres aunque\n",
            "CER: 0.129, WER: 0.600\n",
            "\n",
            "Filename: Paredes transcription_page_64.jpg\n",
            "Ground Truth: algunos no dan por letras a la H, ni a la K, ni a la Y, por dezir que\n",
            "Predicted   : ga ca a afl gar n c de u ate av riterarar aeie\n",
            "CER: 0.681, WER: 0.947\n",
            "\n",
            "Filename: Paredes transcription_page_65.jpg\n",
            "Ground Truth: la H, es solo aspiracion; y hablando en el idoma Latino, y de\n",
            "Predicted   : la h es solo spiracion y hablando en el idoma latino y de\n",
            "CER: 0.098, WER: 0.231\n",
            "\n",
            "Filename: Paredes transcription_page_67.jpg\n",
            "Ground Truth: f. 10v (PDF p. 3)\n",
            "Predicted   : f 0v pdf p 3\n",
            "CER: 0.471, WER: 1.000\n",
            "\n",
            "Filename: Paredes transcription_page_69.jpg\n",
            "Ground Truth: otros estrangeros, dicen bien: mas en el nuestro Castellano no\n",
            "Predicted   : otros ertrangeros dicen bien mas en el nuestro castellano no\n",
            "CER: 0.065, WER: 0.300\n",
            "\n",
            "Filename: Paredes transcription_page_70.jpg\n",
            "Ground Truth: es dudable que en muchos casos tiene tanta fuerza como qual-\n",
            "Predicted   : es eu be qeuden muchos casos tiene tanta fuerza como qual\n",
            "CER: 0.133, WER: 0.364\n",
            "\n",
            "Filename: Paredes transcription_page_71.jpg\n",
            "Ground Truth: quiera de las otras letras, y si no deletreese muchacho, mucho,\n",
            "Predicted   : q ierae e as otras letras y si no deletreese muchacho mucho\n",
            "CER: 0.111, WER: 0.636\n",
            "\n",
            "Filename: Paredes transcription_page_72.jpg\n",
            "Ground Truth: y chaza, y infinitos otros, y se reconocerá claramente: verdad es,\n",
            "Predicted   : y cha ai infinitos otros y se reconocerá claramente verdad es\n",
            "CER: 0.106, WER: 0.455\n",
            "\n",
            "Filename: Paredes transcription_page_73.jpg\n",
            "Ground Truth: que también en el Castellano sirve de aspiracion en muchos ca\n",
            "Predicted   : que uameinun el castellano sirve de aspiracion en muchos ca\n",
            "CER: 0.098, WER: 0.273\n",
            "\n",
            "Filename: Paredes transcription_page_74.jpg\n",
            "Ground Truth: sos como hizo, hazienda, harina, y otros. Excluyen también des-\n",
            "Predicted   : sos cono izo hazienda harina y otros excluyen también des\n",
            "CER: 0.127, WER: 0.700\n",
            "\n",
            "Filename: Paredes transcription_page_75.jpg\n",
            "Ground Truth: te numero la K y la Y, que llamamos Griega, por decir son saca-\n",
            "Predicted   : te numero eore y la y que lamamos griega por decir son saca\n",
            "CER: 0.159, WER: 0.429\n",
            "\n",
            "Filename: Paredes transcription_page_76.jpg\n",
            "Ground Truth: das del Alphabeto Griego, para adorno de las lenguas Latina, y\n",
            "Predicted   : das deb al p abeto griego para adorno de las lenguas latina y\n",
            "CER: 0.129, WER: 0.545\n",
            "\n",
            "Filename: Paredes transcription_page_77.jpg\n",
            "Ground Truth: Castellana, y se ve en la K, que solo sirve para vozes Griegas, co\n",
            "Predicted   : castltanty te ve en la k que solo sirve para vozes griegas co\n",
            "CER: 0.167, WER: 0.357\n",
            "\n",
            "Filename: Paredes transcription_page_78.jpg\n",
            "Ground Truth: mo Kyrie, Kalendas, &c. y de la misma forma en la Y, que se vsa\n",
            "Predicted   : mo kyrie ea endas c y de la misma forma en la y que se vsa\n",
            "CER: 0.143, WER: 0.333\n",
            "\n",
            "Filename: Paredes transcription_page_79.jpg\n",
            "Ground Truth: en algunos nombres estrangeros del Latin. Mas dexando a cada\n",
            "Predicted   : enguongunos nobres estrangeros del latin mas decando a cada\n",
            "CER: 0.150, WER: 0.600\n",
            "\n",
            "Filename: Paredes transcription_page_80.jpg\n",
            "Ground Truth: vno con su parecer, seguiré el comun de todas las Imprentas de\n",
            "Predicted   : vcon su p recer seguiré el comun de todas las imprentas de\n",
            "CER: 0.097, WER: 0.417\n",
            "\n",
            "Filename: Paredes transcription_page_81.jpg\n",
            "Ground Truth: Europa que a vna voz confiessa tener nuestro Abecedario vein-\n",
            "Predicted   : euru qur a vna voz confiessa tener nuestro abecedario vein\n",
            "CER: 0.115, WER: 0.400\n",
            "\n",
            "Filename: Paredes transcription_page_82.jpg\n",
            "Ground Truth: te y tres letras.\n",
            "Predicted   : teee y ytres letras\n",
            "CER: 0.235, WER: 0.750\n",
            "\n",
            "Filename: Paredes transcription_page_83.jpg\n",
            "Ground Truth: Las cinco dellas son a, e, i, o, que llaman vocales, por pro-\n",
            "Predicted   : la s clncecdellas son a e i o que llaman vocales por pro\n",
            "CER: 0.180, WER: 0.692\n",
            "\n",
            "Filename: Paredes transcription_page_84.jpg\n",
            "Ground Truth: nunciarse solo con el aliento de la boca, sin alguna de las qua-\n",
            "Predicted   : nu ui rse solo con el aliento de la boca sin alguna de las qua\n",
            "CER: 0.078, WER: 0.385\n",
            "\n",
            "Filename: Paredes transcription_page_85.jpg\n",
            "Ground Truth: les no se puede formar syllaba: de las otras, nueve son consonan-\n",
            "Predicted   : le ne se e formar syllaba de las otras nuele son consonan\n",
            "CER: 0.154, WER: 0.583\n",
            "\n",
            "Filename: Paredes transcription_page_86.jpg\n",
            "Ground Truth: tes, que son, f, h, l ,m, n, r, s x, z, y la y, que a veces sirve tambien\n",
            "Predicted   : te qe se h l m n r s x z y la y que a veces sirve tambien\n",
            "CER: 0.233, WER: 0.550\n",
            "\n",
            "Filename: Paredes transcription_page_87.jpg\n",
            "Ground Truth: de consonante, porque consuenan; esto es, suenan acompañadas\n",
            "Predicted   : denen ente porque consuenan esto es suenan acompañadas\n",
            "CER: 0.167, WER: 0.500\n",
            "\n",
            "Filename: Paredes transcription_page_88.jpg\n",
            "Ground Truth: de las vocales juntamente con ellas: a las ocho restantes llaman\n",
            "Predicted   : toe et s ta eys cs ey latsatersetyeaertosda adadan\n",
            "CER: 0.688, WER: 1.000\n",
            "\n",
            "Filename: Paredes transcription_page_89.jpg\n",
            "Ground Truth: letras mudas, que son b, c, d, g, K, p, q, t.\n",
            "Predicted   : letsats mudas que son b c d g  p q t\n",
            "CER: 0.267, WER: 0.833\n",
            "\n",
            "Filename: Paredes transcription_page_90.jpg\n",
            "Ground Truth: Las letras grandes se han de poner solo en principio de capi-\n",
            "Predicted   : lasarletrerarandes se han de poner solo en principio de capi\n",
            "CER: 0.131, WER: 0.333\n",
            "\n",
            "Filename: Paredes transcription_page_91.jpg\n",
            "Ground Truth: tulo, o clausula, que es razón que comienza, y al principio de los\n",
            "Predicted   : tulou cauoua que es razón que comienza y al principio de los\n",
            "CER: 0.121, WER: 0.308\n",
            "\n",
            "Filename: Paredes transcription_page_92.jpg\n",
            "Ground Truth: nombres y sobrenombres propios de hombres, Reinos, tierras,\n",
            "Predicted   : norerer y sosrenombres propios de hombres reinos tierras\n",
            "CER: 0.136, WER: 0.625\n",
            "\n",
            "Filename: Paredes transcription_page_93.jpg\n",
            "Ground Truth: montes y ríos, y otras cosas tales, como Alexandro, Andalucia,\n",
            "Predicted   : mentes ysr os y otras cosas tales como alexandro andalucia\n",
            "CER: 0.145, WER: 0.600\n",
            "\n",
            "Filename: Paredes transcription_page_94.jpg\n",
            "Ground Truth: Toledo, Gomez, Moncayo, Tajo &c.\n",
            "Predicted   : toledo gomez moncayo tajo c\n",
            "CER: 0.281, WER: 1.000\n",
            "\n",
            "Filename: Paredes transcription_page_95.jpg\n",
            "Ground Truth: A\tNo ay que decir sobre esta letra, porque siempre se pone\n",
            "Predicted   : an ay qedecir sobre esta letra porque siempre se pone\n",
            "CER: 0.121, WER: 0.417\n",
            "\n",
            "Filename: Paredes transcription_page_96.jpg\n",
            "Ground Truth: como suena, salvo en los casos de ir sola, como dixo Iuan a PE-\n",
            "Predicted   : como sueoa salvo en los casos de ir sola como dixo iuan a pe\n",
            "CER: 0.111, WER: 0.286\n",
            "\n",
            "Filename: Paredes transcription_page_97.jpg\n",
            "Ground Truth: dro de vno a otro, y otros, que siempre estara bien puesta con\n",
            "Predicted   : dro e a otro y otros que siempre estara bien puesta con\n",
            "CER: 0.113, WER: 0.308\n",
            "\n",
            "Filename: Paredes transcription_page_98.jpg\n",
            "Ground Truth: acento grave.\n",
            "Predicted   : acento grave\n",
            "CER: 0.077, WER: 0.500\n",
            "\n",
            "Filename: Paredes transcription_page_99.jpg\n",
            "Ground Truth: B\tEsta es en la que tropiezan mucho los que escriben, y los\n",
            "Predicted   : besta es ens la que tropiezan mucho los que escriben y los\n",
            "CER: 0.085, WER: 0.308\n",
            "\n",
            "Average CER: 0.292\n",
            "Average WER: 0.589\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok\n"
      ],
      "metadata": {
        "id": "suooh--0ZTRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Steamlit Code"
      ],
      "metadata": {
        "id": "JUatc7Y8PRWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill ngrok"
      ],
      "metadata": {
        "id": "CUX_YLcaE7vX"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Specific Task I"
      ],
      "metadata": {
        "id": "BXc62vG6Ex6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "###########################################\n",
        "# Utility Function: Convert Image to Mask\n",
        "###########################################\n",
        "def convert_to_mask(image, threshold=128):\n",
        "    \"\"\"\n",
        "    Convert a PIL image to a binary mask using a grayscale threshold.\n",
        "    Pixels with values above the threshold become 255 (white),\n",
        "    otherwise 0 (black).\n",
        "    \"\"\"\n",
        "    img_gray = image.convert(\"L\")\n",
        "    mask = img_gray.point(lambda p: 255 if p > threshold else 0)\n",
        "    return mask\n",
        "\n",
        "###########################################\n",
        "# Model Building Blocks (matching training code)\n",
        "###########################################\n",
        "# Residual Double Convolution Block\n",
        "class ResDoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ResDoubleConv, self).__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.res_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x if self.res_conv is None else self.res_conv(x)\n",
        "        out = self.double_conv(x)\n",
        "        out += residual\n",
        "        return self.relu(out)\n",
        "\n",
        "# Attention Block for Skip Connections\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, F_g, F_l, F_int):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        self.W_g = nn.Sequential(\n",
        "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(F_int)\n",
        "        )\n",
        "        self.W_x = nn.Sequential(\n",
        "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(F_int)\n",
        "        )\n",
        "        self.psi = nn.Sequential(\n",
        "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, g, x):\n",
        "        g1 = self.W_g(g)\n",
        "        x1 = self.W_x(x)\n",
        "        psi = self.relu(g1 + x1)\n",
        "        psi = self.psi(psi)\n",
        "        return x * psi\n",
        "\n",
        "# Up Block with Attention-Based Skip Connections\n",
        "class UpAttention(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super(UpAttention, self).__init__()\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
        "        self.attention = AttentionBlock(F_g=in_channels // 2, F_l=in_channels // 2, F_int=in_channels // 4)\n",
        "        self.conv = ResDoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x_up, x_skip):\n",
        "        x_up = self.up(x_up)\n",
        "        diffY = x_skip.size()[2] - x_up.size()[2]\n",
        "        diffX = x_skip.size()[3] - x_up.size()[3]\n",
        "        x_up = F.pad(x_up, [diffX // 2, diffX - diffX // 2,\n",
        "                            diffY // 2, diffY - diffY // 2])\n",
        "        x_skip = self.attention(g=x_up, x=x_skip)\n",
        "        x = torch.cat([x_skip, x_up], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "# Positional Encoding for the Transformer\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=1000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(1)  # shape: (max_len, 1, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(0)\n",
        "        return x + self.pe[:seq_len]\n",
        "\n",
        "###########################################\n",
        "# AdvancedTransUNet: Architecture (matching training code)\n",
        "###########################################\n",
        "class AdvancedTransUNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=True, transformer_layers=2, nhead=8):\n",
        "        super(AdvancedTransUNet, self).__init__()\n",
        "        # Encoder\n",
        "        self.inc = ResDoubleConv(n_channels, 64)\n",
        "        self.down1 = nn.Sequential(nn.MaxPool2d(2), ResDoubleConv(64, 128))\n",
        "        self.down2 = nn.Sequential(nn.MaxPool2d(2), ResDoubleConv(128, 256))\n",
        "        self.down3 = nn.Sequential(nn.MaxPool2d(2), ResDoubleConv(256, 512))\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = nn.Sequential(nn.MaxPool2d(2), ResDoubleConv(512, 1024 // factor))\n",
        "        self.feature_dim = 1024 // factor\n",
        "\n",
        "        # Transformer Bottleneck\n",
        "        self.transformer_input_proj = nn.Conv2d(self.feature_dim, self.feature_dim, kernel_size=1)\n",
        "        self.pos_encoder = PositionalEncoding(d_model=self.feature_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=self.feature_dim, nhead=nhead)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=transformer_layers)\n",
        "\n",
        "        # Decoder\n",
        "        self.up1 = UpAttention(in_channels=1024, out_channels=256, bilinear=bilinear)\n",
        "        self.up2 = UpAttention(in_channels=512, out_channels=128, bilinear=bilinear)\n",
        "        self.up3 = UpAttention(in_channels=256, out_channels=64, bilinear=bilinear)\n",
        "        self.up4 = UpAttention(in_channels=128, out_channels=64, bilinear=bilinear)\n",
        "        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x1 = self.inc(x)       # [B, 64, H, W]\n",
        "        x2 = self.down1(x1)    # [B, 128, H/2, W/2]\n",
        "        x3 = self.down2(x2)    # [B, 256, H/4, W/4]\n",
        "        x4 = self.down3(x3)    # [B, 512, H/8, W/8]\n",
        "        x5 = self.down4(x4)    # [B, feature_dim, H/16, W/16]\n",
        "\n",
        "        # Transformer Bottleneck\n",
        "        x5 = self.transformer_input_proj(x5)\n",
        "        B, C, H, W = x5.shape\n",
        "        x5_flat = x5.view(B, C, H * W).permute(2, 0, 1)  # [seq_len, B, C]\n",
        "        x5_flat = self.pos_encoder(x5_flat)\n",
        "        x5_trans = self.transformer(x5_flat)\n",
        "        x5 = x5_trans.permute(1, 2, 0).view(B, C, H, W)\n",
        "\n",
        "        # Decoder\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        return self.outc(x)\n",
        "\n",
        "###########################################\n",
        "# Streamlit App\n",
        "###########################################\n",
        "st.title(\"Advanced TransUNet Layout Segmentation\")\n",
        "st.write(\"Upload a JPG, PNG, or JPRG image. The image will be converted to a binary mask and processed for segmentation.\")\n",
        "\n",
        "# Allow only jpg, png, or jprg files.\n",
        "uploaded_file = st.file_uploader(\"Choose an image file\", type=[\"jpg\", \"jpeg\", \"png\", \"jprg\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # Open the image and display the original.\n",
        "    image = Image.open(uploaded_file).convert(\"RGB\")\n",
        "    st.subheader(\"Original Image\")\n",
        "    st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n",
        "\n",
        "    # Convert the image to a binary mask.\n",
        "    mask_image = convert_to_mask(image, threshold=128)\n",
        "    st.subheader(\"Converted Binary Mask\")\n",
        "    st.image(mask_image, caption=\"Binary Mask (Threshold = 128)\", use_column_width=True)\n",
        "\n",
        "    # Preprocess the original image for the model (resize to 256x256 and convert to tensor).\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    image_tensor = transform(image).unsqueeze(0)  # Shape: (1, 3, 256, 256)\n",
        "\n",
        "    # Load the pretrained AdvancedTransUNet model.\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = AdvancedTransUNet(n_channels=3, n_classes=2, bilinear=True, transformer_layers=2, nhead=8).to(device)\n",
        "\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(\"advanced_transunet_model.pth\", map_location=device))\n",
        "        model.eval()\n",
        "    except Exception as e:\n",
        "        st.error(f\"Failed to load model: {e}\")\n",
        "\n",
        "    # Run the prediction.\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image_tensor.to(device))  # Output shape: [1, 2, 256, 256]\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    # Convert prediction to displayable image.\n",
        "    preds_np = preds.squeeze(0).cpu().numpy()\n",
        "    predicted_mask = Image.fromarray((preds_np * 255).astype('uint8'))\n",
        "\n",
        "    st.subheader(\"Predicted Segmentation Mask\")\n",
        "    st.image(predicted_mask, caption=\"Predicted Mask\", use_column_width=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0P3DwCUSd1m",
        "outputId": "eeb1bda5-3015-4355-92fd-1a6097ab18db"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2tRxo1f5xz3Q6ZLAO1ZvYXGk3Uh_cTghpPB9TeLmgm57x56n"
      ],
      "metadata": {
        "id": "Ocs6_h2xDWxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Kill all existing ngrok tunnels to avoid exceeding the limit.\n",
        "ngrok.kill()\n",
        "\n",
        "# Set your ngrok auth token (replace with your own)\n",
        "ngrok.set_auth_token(\"2tRxo1f5xz3Q6ZLAO1ZvYXGk3Uh_cTghpPB9TeLmgm57x56n\")\n",
        "\n",
        "# Launch the Streamlit app in the background (only needed in a notebook environment)\n",
        "if \"get_ipython\" in globals():\n",
        "    get_ipython().system_raw(\"streamlit run app.py &\")\n",
        "\n",
        "# Create an ngrok tunnel to the default Streamlit port 8501\n",
        "public_url = ngrok.connect(addr=\"8501\", proto=\"http\")\n",
        "print(\"Streamlit app is live at:\", public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Uga0R7ooGTg",
        "outputId": "917dceee-b080-42ac-e17f-33c886f70d75"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app is live at: NgrokTunnel: \"https://4241-34-86-253-66.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}